# import pandas as pd
# import numpy as np
# from sklearn.linear_model import HuberRegressor
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import mean_squared_error, r2_score
#
# # ========= 1. è¯»å–æ•°æ® =========
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ========= 2. åˆ—å®šä¹‰ =========
# group_cols = df.columns[11:30]   # 19ä¸ªåŸºå›¢åˆ—
# temp_cols  = df.columns[30:40]   # 10ä¸ªæ¸©åº¦ç‚¹
# cp_cols    = df.columns[40:50]   # 10ä¸ª Cp å€¼
# target_column_T1 = 'ASPEN Half Critical T'  # çœŸå® T1 æ‰€åœ¨åˆ—
#
# # ä½ ç»™å‡ºçš„â€œçœŸå®å››åˆ—â€ï¼šCp1_true=ç¬¬10åˆ—, Cp2_true=ç¬¬51åˆ—, T1_true=target_column_T1, T2_true=1.5*T1_true
# CP1_TRUE_IDX = 9
# CP2_TRUE_IDX = 50
# T1_TRUE_COL  = target_column_T1
#
# # ========= 2.1 å¼ºåˆ¶æ•°å€¼åŒ–ï¼ˆå…³é”®ä¿®æ­£ï¼‰=========
# # å°†ç”¨äºå»ºæ¨¡/è®¡ç®—çš„åˆ—å…¨éƒ¨è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è§£æçš„è®¾ä¸º NaN
# cols_to_numeric = list(group_cols) + list(temp_cols) + list(cp_cols) + [T1_TRUE_COL]
# # æ³¨æ„ iloc ä¸¤åˆ—éœ€è¦å•ç‹¬å¤„ç†åå†™å›
# df[group_cols] = df[group_cols].apply(pd.to_numeric, errors="coerce")
# df[temp_cols]  = df[temp_cols].apply(pd.to_numeric, errors="coerce")
# df[cp_cols]    = df[cp_cols].apply(pd.to_numeric, errors="coerce")
#
# # æŠŠç¬¬10ã€51åˆ—ä¹Ÿæ•°å€¼åŒ–
# df.iloc[:, CP1_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP1_TRUE_IDX], errors="coerce")
# df.iloc[:, CP2_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP2_TRUE_IDX], errors="coerce")
# df[T1_TRUE_COL]          = pd.to_numeric(df[T1_TRUE_COL], errors="coerce")
#
# # ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
# X_groups = df[group_cols]
# # T1 è®­ç»ƒæœ‰æ•ˆæ©ç 
# valid_mask = X_groups.notna().all(1) & df[T1_TRUE_COL].notna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# # ç”¨ GradientBoostingRegressor é¢„æµ‹ T1ï¼ˆä¸ä½ åŸæœ‰è®¾ç½®ä¸€è‡´ï¼‰
# y_T1 = df.loc[valid_mask, T1_TRUE_COL].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
# ).fit(X_poly, y_T1)
#
# # Cp1, Cp2 å­æ¨¡å‹ï¼ˆç”¨é¢å¤–ä¸¤åˆ—çš„çœŸå® Cpï¼‰
# Cp1_true_series = df.iloc[:, CP1_TRUE_IDX]
# Cp2_true_series = df.iloc[:, CP2_TRUE_IDX]
# valid_cp_mask = X_groups.notna().all(1) & Cp1_true_series.notna() & Cp2_true_series.notna()
#
# Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp1_true_series[valid_cp_mask].values)
# Cp2_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp2_true_series[valid_cp_mask].values)
#
# # ========= 3.1 å­æ¨¡å‹è¯„ä¼°ï¼ˆin-sampleï¼‰=========
# y_pred_T1 = T1_model.predict(X_poly)
# r2_T1 = r2_score(y_T1, y_pred_T1)
# mse_T1 = mean_squared_error(y_T1, y_pred_T1)
#
# y_Cp1_true = Cp1_true_series[valid_cp_mask]
# y_Cp1_pred = Cp1_model.predict(X_groups[valid_cp_mask].values)
# r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred)
# mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred)
#
# y_Cp2_true = Cp2_true_series[valid_cp_mask]
# y_Cp2_pred = Cp2_model.predict(X_groups[valid_cp_mask].values)
# r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred)
# mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred)
#
# print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
# print(f"T1_model ->     RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
# print(f"Cp1_model ->    RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
# print(f"Cp2_model ->    RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")
#
# # ========= 4. æ„å»ºè®­ç»ƒæ•°æ®ï¼ˆä¸¤ç§ slope å˜ä½“ï¼‰=========
# X_A, y_A, id_A, T_A = [], [], [], []  # Aï¼šçœŸå®Î”Cp / é¢„æµ‹Î”T
# X_B, y_B, id_B, T_B = [], [], [], []  # Bï¼šé¢„æµ‹Î”Cp / çœŸå®Î”T
#
# # å¯¹â€œæ‰€æœ‰è¡Œâ€çš„åŸºå›¢ç‰¹å¾åš transformï¼ˆpoly ä¹‹å‰åœ¨ valid_mask ä¸Š fit è¿‡ï¼‰
# X_poly_all = poly.transform(X_groups.fillna(0))  # è¿™é‡Œ transform ä¸ä¼šå¼•å…¥ NaNï¼›æ¨¡å‹é¢„æµ‹å‰ä»ä¼šè¿‡æ»¤
#
# for i, row in df.iterrows():
#     try:
#         material_id = row.iloc[0]
#
#         # --- å– Nkï¼Œç¡®ä¿æ•°å€¼ & æ£€æŸ¥ç¼ºå¤± ---
#         Nk_series = row[group_cols].astype(float)
#         if pd.isna(Nk_series).any():
#             continue
#         Nk = Nk_series.values
#
#         # --- é¢„æµ‹ä¾§ T1/T2 ä¸ Cp1/Cp2 ---
#         # ç”¨ä¸ç¬¬ i è¡Œå¯¹é½çš„å¤šé¡¹å¼ç‰¹å¾ï¼ˆå·²ç» transform å¥½ï¼‰
#         T1_pred = float(T1_model.predict(X_poly_all[i:i+1])[0])
#         if not np.isfinite(T1_pred) or T1_pred <= 0:
#             continue
#         T2_pred = 1.5 * T1_pred
#
#         Nk_df = pd.DataFrame([Nk], columns=group_cols)
#         Cp1_pred = float(Cp1_model.predict(Nk_df.values)[0])
#         Cp2_pred = float(Cp2_model.predict(Nk_df.values)[0])
#         if not (np.isfinite(Cp1_pred) and np.isfinite(Cp2_pred)):
#             continue
#
#         # --- çœŸå®ä¾§ Cp1/Cp2/T1/T2 ---
#         Cp1_true = row.iloc[CP1_TRUE_IDX]
#         Cp2_true = row.iloc[CP2_TRUE_IDX]
#         T1_true  = row[T1_TRUE_COL]
#         if not (np.isfinite(Cp1_true) and np.isfinite(Cp2_true) and np.isfinite(T1_true)):
#             continue
#         T2_true  = 1.5 * T1_true
#
#         # é˜²æ­¢é™¤é›¶
#         if T2_pred == T1_pred or T2_true == T1_true:
#             continue
#
#         # --- ä¸¤ç§ slope å˜ä½“ ---
#         slope_A = (Cp2_true - Cp1_true) / (T2_pred - T1_pred)   # åˆ†å­çœŸå®Î”Cpï¼Œåˆ†æ¯é¢„æµ‹Î”T
#         slope_B = (Cp2_pred - Cp1_pred) / (T2_true - T1_true)   # åˆ†å­é¢„æµ‹Î”Cpï¼Œåˆ†æ¯çœŸå®Î”T
#         if not (np.isfinite(slope_A) and np.isfinite(slope_B)):
#             continue
#
#         # --- é€æ¸©åº¦ç‚¹å±•å¼€ ---
#         temps = row[temp_cols].astype(float).values
#         cps   = row[cp_cols].astype(float).values
#         # æ©ç ï¼šå»é™¤ NaN
#         mask_pts = np.isfinite(temps) & np.isfinite(cps)
#         if not mask_pts.any():
#             continue
#
#         for T, Cp in zip(temps[mask_pts], cps[mask_pts]):
#             feats_A = np.concatenate([Nk, Nk*T, [slope_A*T]])
#             feats_B = np.concatenate([Nk, Nk*T, [slope_B*T]])
#             X_A.append(feats_A); y_A.append(Cp); id_A.append(material_id); T_A.append(T)
#             X_B.append(feats_B); y_B.append(Cp); id_B.append(material_id); T_B.append(T)
#
#     except Exception as e:
#         print(f"[WARN] row {i} skipped: {e}")
#         continue
#
# X_A = np.asarray(X_A); y_A = np.asarray(y_A)
# X_B = np.asarray(X_B); y_B = np.asarray(y_B)
#
# if X_A.size == 0 or X_B.size == 0:
#     raise RuntimeError(
#         f"æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼šX_A.shape={X_A.shape}, X_B.shape={X_B.shape}ã€‚"
#         "è¯·æ£€æŸ¥ group/temp/cp åˆ—æ˜¯å¦ä¸ºæ•°å€¼ã€ä»¥åŠçœŸå®åˆ—æ˜¯å¦å­˜åœ¨ç¼ºå¤±ã€‚"
#     )
#
# # ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰=========
# model_A = HuberRegressor(max_iter=10000).fit(X_A, y_A)
# model_B = HuberRegressor(max_iter=10000).fit(X_B, y_B)
#
# # ========= 6. è¯„ä¼° =========
# def eval_and_print(tag, model, X, y):
#     y_pred = model.predict(X)
#     mse = mean_squared_error(y, y_pred)
#     r2 = r2_score(y, y_pred)
#     ard = np.mean(np.abs((y - y_pred) / y)) * 100
#     rel_err = np.abs((y_pred - y) / y) * 100
#     within_1pct  = int((rel_err <= 1).sum())
#     within_5pct  = int((rel_err <= 5).sum())
#     within_10pct = int((rel_err <= 10).sum())
#
#     print(f"\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆ{tag}ï¼‰ï¼š")
#     print(f"RÂ²  = {r2:.4f}")
#     print(f"MSE = {mse:.2f}")
#     print(f"ARD = {ard:.2f}%")
#     print(f"âœ… è¯¯å·® â‰¤ 1% : {within_1pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 5% : {within_5pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 10%: {within_10pct}")
#     return y_pred
#
# y_pred_A = eval_and_print("A=çœŸå®Î”Cp / é¢„æµ‹Î”T", model_A, X_A, y_A)
# y_pred_B = eval_and_print("B=é¢„æµ‹Î”Cp / çœŸå®Î”T", model_B, X_B, y_B)
#
# # ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
# results_A = pd.DataFrame({
#     "Material_ID": id_A,
#     "Temperature (K)": T_A,
#     "Cp_measured": y_A,
#     "Cp_predicted": y_pred_A
# })
# results_B = pd.DataFrame({
#     "Material_ID": id_B,
#     "Temperature (K)": T_B,
#     "Cp_measured": y_B,
#     "Cp_predicted": y_pred_B
# })
# results_A.to_excel("Cpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# results_B.to_excel("Cpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
# print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
# print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
#
# # ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
# feature_labels = (
#     list(group_cols) +                # 19 ä¸ªåŸºå›¢
#     [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
#     ["slopeÃ—T"]                       # 1 ä¸ªæ–°ç‰¹å¾
# )
# coef_A = pd.DataFrame({"Feature": feature_labels, "Contribution": model_A.coef_})
# coef_B = pd.DataFrame({"Feature": feature_labels, "Contribution": model_B.coef_})
# coef_A.to_excel("Cpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# coef_B.to_excel("Cpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
# print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
# print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
# import pandas as pd
# import numpy as np
# from sklearn.linear_model import HuberRegressor
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import mean_squared_error, r2_score
#
# # ========= 1. è¯»å–æ•°æ® =========
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ========= 2. åˆ—å®šä¹‰ =========
# group_cols = df.columns[11:30]   # 19ä¸ªåŸºå›¢åˆ—
# temp_cols  = df.columns[30:40]   # 10ä¸ªæ¸©åº¦ç‚¹
# cp_cols    = df.columns[40:50]   # 10ä¸ª Cp å€¼
# target_column_T1 = 'ASPEN Half Critical T'  # çœŸå® T1 æ‰€åœ¨åˆ—
#
# # ä½ ç»™å‡ºçš„â€œçœŸå®å››åˆ—â€ï¼šCp1_true=ç¬¬10åˆ—, Cp2_true=ç¬¬51åˆ—, T1_true=target_column_T1, T2_true=1.5*T1_true
# CP1_TRUE_IDX = 9
# CP2_TRUE_IDX = 50
# T1_TRUE_COL  = target_column_T1
#
# # ========= 2.1 å¼ºåˆ¶æ•°å€¼åŒ–ï¼ˆå…³é”®ä¿®æ­£ï¼‰=========
# # å°†ç”¨äºå»ºæ¨¡/è®¡ç®—çš„åˆ—å…¨éƒ¨è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è§£æçš„è®¾ä¸º NaN
# df[group_cols] = df[group_cols].apply(pd.to_numeric, errors="coerce")
# df[temp_cols]  = df[temp_cols].apply(pd.to_numeric, errors="coerce")
# df[cp_cols]    = df[cp_cols].apply(pd.to_numeric, errors="coerce")
# # æŠŠç¬¬10ã€51åˆ—ä¹Ÿæ•°å€¼åŒ–
# df.iloc[:, CP1_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP1_TRUE_IDX], errors="coerce")
# df.iloc[:, CP2_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP2_TRUE_IDX], errors="coerce")
# df[T1_TRUE_COL]          = pd.to_numeric(df[T1_TRUE_COL], errors="coerce")
#
# # ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
# X_groups = df[group_cols]
# # T1 è®­ç»ƒæœ‰æ•ˆæ©ç 
# valid_mask = X_groups.notna().all(1) & df[T1_TRUE_COL].notna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# # ç”¨ GradientBoostingRegressor é¢„æµ‹ T1ï¼ˆä¸ä½ åŸæœ‰è®¾ç½®ä¸€è‡´ï¼‰
# y_T1 = df.loc[valid_mask, T1_TRUE_COL].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
# ).fit(X_poly, y_T1)
#
# # Cp1, Cp2 å­æ¨¡å‹ï¼ˆç”¨é¢å¤–ä¸¤åˆ—çš„çœŸå® Cpï¼‰
# Cp1_true_series = df.iloc[:, CP1_TRUE_IDX]
# Cp2_true_series = df.iloc[:, CP2_TRUE_IDX]
# valid_cp_mask = X_groups.notna().all(1) & Cp1_true_series.notna() & Cp2_true_series.notna()
#
# Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp1_true_series[valid_cp_mask].values)
# Cp2_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp2_true_series[valid_cp_mask].values)
#
# # ========= 3.1 å­æ¨¡å‹è¯„ä¼°ï¼ˆin-sampleï¼‰=========
# y_pred_T1 = T1_model.predict(X_poly)
# r2_T1 = r2_score(y_T1, y_pred_T1)
# mse_T1 = mean_squared_error(y_T1, y_pred_T1)
#
# y_Cp1_true = Cp1_true_series[valid_cp_mask]
# y_Cp1_pred = Cp1_model.predict(X_groups[valid_cp_mask].values)
# r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred)
# mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred)
#
# y_Cp2_true = Cp2_true_series[valid_cp_mask]
# y_Cp2_pred = Cp2_model.predict(X_groups[valid_cp_mask].values)
# r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred)
# mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred)
#
# print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
# print(f"T1_model ->     RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
# print(f"Cp1_model ->    RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
# print(f"Cp2_model ->    RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")
#
# # ========= 4. æ„å»ºè®­ç»ƒæ•°æ®ï¼ˆA/B/C ä¸‰ç§ slope å˜ä½“ï¼‰=========
# X_A, y_A, id_A, T_A = [], [], [], []  # Aï¼šçœŸå®Î”Cp / é¢„æµ‹Î”T
# X_B, y_B, id_B, T_B = [], [], [], []  # Bï¼šé¢„æµ‹Î”Cp / çœŸå®Î”T
# X_C, y_C, id_C, T_C = [], [], [], []  # Cï¼šé¢„æµ‹Î”Cp / é¢„æµ‹Î”Tï¼ˆå…¨é¢„æµ‹ï¼‰
#
# # å¯¹â€œæ‰€æœ‰è¡Œâ€çš„åŸºå›¢ç‰¹å¾åš transformï¼ˆpoly ä¹‹å‰åœ¨ valid_mask ä¸Š fit è¿‡ï¼‰
# X_poly_all = poly.transform(X_groups.fillna(0))  # è¿™é‡Œ transform ä¸ä¼šå¼•å…¥ NaN
#
# for i, row in df.iterrows():
#     try:
#         material_id = row.iloc[0]
#
#         # --- å– Nkï¼Œç¡®ä¿æ•°å€¼ & æ£€æŸ¥ç¼ºå¤± ---
#         Nk_series = row[group_cols].astype(float)
#         if pd.isna(Nk_series).any():
#             continue
#         Nk = Nk_series.values
#
#         # --- é¢„æµ‹ä¾§ T1/T2 ä¸ Cp1/Cp2 ---
#         T1_pred = float(T1_model.predict(X_poly_all[i:i+1])[0])
#         if not np.isfinite(T1_pred) or T1_pred <= 0:
#             continue
#         T2_pred = 1.5 * T1_pred
#
#         Nk_df_vals = pd.DataFrame([Nk], columns=group_cols).values
#         Cp1_pred = float(Cp1_model.predict(Nk_df_vals)[0])
#         Cp2_pred = float(Cp2_model.predict(Nk_df_vals)[0])
#         if not (np.isfinite(Cp1_pred) and np.isfinite(Cp2_pred)):
#             continue
#
#         # --- çœŸå®ä¾§ Cp1/Cp2/T1/T2 ---
#         Cp1_true = row.iloc[CP1_TRUE_IDX]
#         Cp2_true = row.iloc[CP2_TRUE_IDX]
#         T1_true  = row[T1_TRUE_COL]
#         if not (np.isfinite(Cp1_true) and np.isfinite(Cp2_true) and np.isfinite(T1_true)):
#             continue
#         T2_true  = 1.5 * T1_true
#
#         # é˜²æ­¢é™¤é›¶
#         if T2_pred == T1_pred or T2_true == T1_true:
#             continue
#
#         # --- ä¸‰ç§ slope å˜ä½“ ---
#         slope_A = (Cp2_true - Cp1_true) / (T2_pred - T1_pred)   # Aï¼šåˆ†å­çœŸå®Î”Cpï¼Œåˆ†æ¯é¢„æµ‹Î”T
#         slope_B = (Cp2_pred - Cp1_pred) / (T2_true - T1_true)   # Bï¼šåˆ†å­é¢„æµ‹Î”Cpï¼Œåˆ†æ¯çœŸå®Î”T
#         slope_C = (Cp2_pred - Cp1_pred) / (T2_pred - T1_pred)   # Cï¼šåˆ†å­é¢„æµ‹Î”Cpï¼Œåˆ†æ¯é¢„æµ‹Î”Tï¼ˆå…¨é¢„æµ‹ï¼‰
#         if not (np.isfinite(slope_A) and np.isfinite(slope_B) and np.isfinite(slope_C)):
#             continue
#
#         # --- é€æ¸©åº¦ç‚¹å±•å¼€ ---
#         temps = row[temp_cols].astype(float).values
#         cps   = row[cp_cols].astype(float).values
#         mask_pts = np.isfinite(temps) & np.isfinite(cps)
#         if not mask_pts.any():
#             continue
#
#         for T, Cp in zip(temps[mask_pts], cps[mask_pts]):
#             feats_A = np.concatenate([Nk, Nk*T, [slope_A*T]])
#             feats_B = np.concatenate([Nk, Nk*T, [slope_B*T]])
#             feats_C = np.concatenate([Nk, Nk*T, [slope_C*T]])
#             X_A.append(feats_A); y_A.append(Cp); id_A.append(material_id); T_A.append(T)
#             X_B.append(feats_B); y_B.append(Cp); id_B.append(material_id); T_B.append(T)
#             X_C.append(feats_C); y_C.append(Cp); id_C.append(material_id); T_C.append(T)
#
#     except Exception as e:
#         print(f"[WARN] row {i} skipped: {e}")
#         continue
#
# X_A = np.asarray(X_A); y_A = np.asarray(y_A)
# X_B = np.asarray(X_B); y_B = np.asarray(y_B)
# X_C = np.asarray(X_C); y_C = np.asarray(y_C)
#
# if X_A.size == 0 or X_B.size == 0 or X_C.size == 0:
#     raise RuntimeError(
#         f"æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼šX_A{X_A.shape}, X_B{X_B.shape}, X_C{X_C.shape}ã€‚"
#         "è¯·æ£€æŸ¥ group/temp/cp åˆ—æ˜¯å¦ä¸ºæ•°å€¼ã€ä»¥åŠçœŸå®/é¢„æµ‹åˆ—æ˜¯å¦å­˜åœ¨ç¼ºå¤±ã€‚"
#     )
#
# # ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰=========
# model_A = HuberRegressor(max_iter=10000).fit(X_A, y_A)
# model_B = HuberRegressor(max_iter=10000).fit(X_B, y_B)
# model_C = HuberRegressor(max_iter=10000).fit(X_C, y_C)
#
# # ========= 6. è¯„ä¼° =========
# def eval_and_print(tag, model, X, y):
#     y_pred = model.predict(X)
#     mse = mean_squared_error(y, y_pred)
#     r2 = r2_score(y, y_pred)
#     ard = np.mean(np.abs((y - y_pred) / y)) * 100
#     rel_err = np.abs((y_pred - y) / y) * 100
#     within_1pct  = int((rel_err <= 1).sum())
#     within_5pct  = int((rel_err <= 5).sum())
#     within_10pct = int((rel_err <= 10).sum())
#
#     print(f"\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆ{tag}ï¼‰ï¼š")
#     print(f"RÂ²  = {r2:.4f}")
#     print(f"MSE = {mse:.2f}")
#     print(f"ARD = {ard:.2f}%")
#     print(f"âœ… è¯¯å·® â‰¤ 1% : {within_1pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 5% : {within_5pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 10%: {within_10pct}")
#     return y_pred
#
# y_pred_A = eval_and_print("A=çœŸå®Î”Cp / é¢„æµ‹Î”T", model_A, X_A, y_A)
# y_pred_B = eval_and_print("B=é¢„æµ‹Î”Cp / çœŸå®Î”T", model_B, X_B, y_B)
# y_pred_C = eval_and_print("C=é¢„æµ‹Î”Cp / é¢„æµ‹Î”T", model_C, X_C, y_C)
#
# # ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
# results_A = pd.DataFrame({
#     "Material_ID": id_A,
#     "Temperature (K)": T_A,
#     "Cp_measured": y_A,
#     "Cp_predicted": y_pred_A
# })
# results_B = pd.DataFrame({
#     "Material_ID": id_B,
#     "Temperature (K)": T_B,
#     "Cp_measured": y_B,
#     "Cp_predicted": y_pred_B
# })
# results_C = pd.DataFrame({
#     "Material_ID": id_C,
#     "Temperature (K)": T_C,
#     "Cp_measured": y_C,
#     "Cp_predicted": y_pred_C
# })
#
# results_A.to_excel("Cpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# results_B.to_excel("Cpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
# results_C.to_excel("Cpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
# print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
# print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx")
#
# # ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
# feature_labels = (
#     list(group_cols) +                # 19 ä¸ªåŸºå›¢
#     [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
#     ["slopeÃ—T"]                       # 1 ä¸ªæ–°ç‰¹å¾
# )
# coef_A = pd.DataFrame({"Feature": feature_labels, "Contribution": model_A.coef_})
# coef_B = pd.DataFrame({"Feature": feature_labels, "Contribution": model_B.coef_})
# coef_C = pd.DataFrame({"Feature": feature_labels, "Contribution": model_C.coef_})
#
# coef_A.to_excel("Cpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# coef_B.to_excel("Cpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
# coef_C.to_excel("Cpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
# print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
# print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
# print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx")


import pandas as pd
import numpy as np
from sklearn.linear_model import HuberRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

# ========= 1. è¯»å–æ•°æ® =========
file_path = "heat capacity 207.xlsx"
df = pd.read_excel(file_path, sheet_name="Sheet1")
df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ========= 2. åˆ—å®šä¹‰ =========
group_cols = df.columns[11:30]  # 19ä¸ªåŸºå›¢åˆ—
temp_cols = df.columns[30:40]  # 10ä¸ªæ¸©åº¦ç‚¹
cp_cols = df.columns[40:50]  # 10ä¸ª Cp å€¼
target_column_T1 = 'ASPEN Half Critical T'  # çœŸå® T1 æ‰€åœ¨åˆ—

# ä½ ç»™å‡ºçš„"çœŸå®å››åˆ—"ï¼šCp1_true=ç¬¬10åˆ—, Cp2_true=ç¬¬51åˆ—, T1_true=target_column_T1, T2_true=1.5*T1_true
CP1_TRUE_IDX = 9
CP2_TRUE_IDX = 50
T1_TRUE_COL = target_column_T1

# ========= 2.1 å¼ºåˆ¶æ•°å€¼åŒ–ï¼ˆå…³é”®ä¿®æ­£ï¼‰=========
# å°†ç”¨äºå»ºæ¨¡/è®¡ç®—çš„åˆ—å…¨éƒ¨è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è§£æçš„è®¾ä¸º NaN
df[group_cols] = df[group_cols].apply(pd.to_numeric, errors="coerce")
df[temp_cols] = df[temp_cols].apply(pd.to_numeric, errors="coerce")
df[cp_cols] = df[cp_cols].apply(pd.to_numeric, errors="coerce")
# æŠŠç¬¬10ã€51åˆ—ä¹Ÿæ•°å€¼åŒ–
df.iloc[:, CP1_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP1_TRUE_IDX], errors="coerce")
df.iloc[:, CP2_TRUE_IDX] = pd.to_numeric(df.iloc[:, CP2_TRUE_IDX], errors="coerce")
df[T1_TRUE_COL] = pd.to_numeric(df[T1_TRUE_COL], errors="coerce")

# ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
X_groups = df[group_cols]
# T1 è®­ç»ƒæœ‰æ•ˆæ©ç 
valid_mask = X_groups.notna().all(1) & df[T1_TRUE_COL].notna()

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_groups[valid_mask])

# ç”¨ GradientBoostingRegressor é¢„æµ‹ T1ï¼ˆä¸ä½ åŸæœ‰è®¾ç½®ä¸€è‡´ï¼‰
y_T1 = df.loc[valid_mask, T1_TRUE_COL].values
T1_model = GradientBoostingRegressor(
    n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
).fit(X_poly, y_T1)

# Cp1, Cp2 å­æ¨¡å‹ï¼ˆç”¨é¢å¤–ä¸¤åˆ—çš„çœŸå® Cpï¼‰
Cp1_true_series = df.iloc[:, CP1_TRUE_IDX]
Cp2_true_series = df.iloc[:, CP2_TRUE_IDX]
valid_cp_mask = X_groups.notna().all(1) & Cp1_true_series.notna() & Cp2_true_series.notna()

Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp1_true_series[valid_cp_mask].values)
Cp2_model = HuberRegressor(max_iter=9000).fit(X_groups[valid_cp_mask].values, Cp2_true_series[valid_cp_mask].values)

# ========= 3.1 å­æ¨¡å‹è¯„ä¼°ï¼ˆin-sampleï¼‰=========
y_pred_T1 = T1_model.predict(X_poly)
r2_T1 = r2_score(y_T1, y_pred_T1)
mse_T1 = mean_squared_error(y_T1, y_pred_T1)

y_Cp1_true = Cp1_true_series[valid_cp_mask]
y_Cp1_pred = Cp1_model.predict(X_groups[valid_cp_mask].values)
r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred)
mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred)

y_Cp2_true = Cp2_true_series[valid_cp_mask]
y_Cp2_pred = Cp2_model.predict(X_groups[valid_cp_mask].values)
r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred)
mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred)

print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
print(f"T1_model ->     RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
print(f"Cp1_model ->    RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
print(f"Cp2_model ->    RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")

# ========= 4. æ„å»ºè®­ç»ƒæ•°æ®ï¼ˆA/B/C/D å››ç§ slope å˜ä½“ï¼‰=========
X_A, y_A, id_A, T_A = [], [], [], []  # Aï¼šçœŸå®Î”Cp / é¢„æµ‹Î”T
X_B, y_B, id_B, T_B = [], [], [], []  # Bï¼šé¢„æµ‹Î”Cp / çœŸå®Î”T
X_C, y_C, id_C, T_C = [], [], [], []  # Cï¼šé¢„æµ‹Î”Cp / é¢„æµ‹Î”Tï¼ˆå…¨é¢„æµ‹ï¼‰
X_D, y_D, id_D, T_D = [], [], [], []  # Dï¼šçœŸå®Î”Cp / çœŸå®Î”Tï¼ˆå®Œå…¨çœŸå®ï¼‰

# å¯¹"æ‰€æœ‰è¡Œ"çš„åŸºå›¢ç‰¹å¾åš transformï¼ˆpoly ä¹‹å‰åœ¨ valid_mask ä¸Š fit è¿‡ï¼‰
X_poly_all = poly.transform(X_groups.fillna(0))  # è¿™é‡Œ transform ä¸ä¼šå¼•å…¥ NaN

for i, row in df.iterrows():
    try:
        material_id = row.iloc[0]

        # --- å– Nkï¼Œç¡®ä¿æ•°å€¼ & æ£€æŸ¥ç¼ºå¤± ---
        Nk_series = row[group_cols].astype(float)
        if pd.isna(Nk_series).any():
            continue
        Nk = Nk_series.values

        # --- é¢„æµ‹ä¾§ T1/T2 ä¸ Cp1/Cp2 ---
        T1_pred = float(T1_model.predict(X_poly_all[i:i + 1])[0])
        if not np.isfinite(T1_pred) or T1_pred <= 0:
            continue
        T2_pred = 1.5 * T1_pred

        Nk_df_vals = pd.DataFrame([Nk], columns=group_cols).values
        Cp1_pred = float(Cp1_model.predict(Nk_df_vals)[0])
        Cp2_pred = float(Cp2_model.predict(Nk_df_vals)[0])
        if not (np.isfinite(Cp1_pred) and np.isfinite(Cp2_pred)):
            continue

        # --- çœŸå®ä¾§ Cp1/Cp2/T1/T2 ---
        Cp1_true = row.iloc[CP1_TRUE_IDX]
        Cp2_true = row.iloc[CP2_TRUE_IDX]
        T1_true = row[T1_TRUE_COL]
        if not (np.isfinite(Cp1_true) and np.isfinite(Cp2_true) and np.isfinite(T1_true)):
            continue
        T2_true = 1.5 * T1_true

        # é˜²æ­¢é™¤é›¶
        if T2_pred == T1_pred or T2_true == T1_true:
            continue

        # --- å››ç§ slope å˜ä½“ ---
        slope_A = (Cp2_true - Cp1_true) / (T2_pred - T1_pred)  # Aï¼šåˆ†å­çœŸå®Î”Cpï¼Œåˆ†æ¯é¢„æµ‹Î”T
        slope_B = (Cp2_pred - Cp1_pred) / (T2_true - T1_true)  # Bï¼šåˆ†å­é¢„æµ‹Î”Cpï¼Œåˆ†æ¯çœŸå®Î”T
        slope_C = (Cp2_pred - Cp1_pred) / (T2_pred - T1_pred)  # Cï¼šåˆ†å­é¢„æµ‹Î”Cpï¼Œåˆ†æ¯é¢„æµ‹Î”Tï¼ˆå…¨é¢„æµ‹ï¼‰
        slope_D = (Cp2_true - Cp1_true) / (T2_true - T1_true)  # Dï¼šåˆ†å­çœŸå®Î”Cpï¼Œåˆ†æ¯çœŸå®Î”Tï¼ˆå®Œå…¨çœŸå®ï¼‰

        if not (np.isfinite(slope_A) and np.isfinite(slope_B) and
                np.isfinite(slope_C) and np.isfinite(slope_D)):
            continue

        # --- é€æ¸©åº¦ç‚¹å±•å¼€ ---
        temps = row[temp_cols].astype(float).values
        cps = row[cp_cols].astype(float).values
        mask_pts = np.isfinite(temps) & np.isfinite(cps)
        if not mask_pts.any():
            continue

        for T, Cp in zip(temps[mask_pts], cps[mask_pts]):
            feats_A = np.concatenate([Nk, Nk * T, [slope_A * T]])
            feats_B = np.concatenate([Nk, Nk * T, [slope_B * T]])
            feats_C = np.concatenate([Nk, Nk * T, [slope_C * T]])
            feats_D = np.concatenate([Nk, Nk * T, [slope_D * T]])  # æ–°å¢Då˜ä½“

            X_A.append(feats_A);
            y_A.append(Cp);
            id_A.append(material_id);
            T_A.append(T)
            X_B.append(feats_B);
            y_B.append(Cp);
            id_B.append(material_id);
            T_B.append(T)
            X_C.append(feats_C);
            y_C.append(Cp);
            id_C.append(material_id);
            T_C.append(T)
            X_D.append(feats_D);
            y_D.append(Cp);
            id_D.append(material_id);
            T_D.append(T)  # æ–°å¢Då˜ä½“

    except Exception as e:
        print(f"[WARN] row {i} skipped: {e}")
        continue

X_A = np.asarray(X_A);
y_A = np.asarray(y_A)
X_B = np.asarray(X_B);
y_B = np.asarray(y_B)
X_C = np.asarray(X_C);
y_C = np.asarray(y_C)
X_D = np.asarray(X_D);
y_D = np.asarray(y_D)  # æ–°å¢Då˜ä½“

if X_A.size == 0 or X_B.size == 0 or X_C.size == 0 or X_D.size == 0:
    raise RuntimeError(
        f"æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼šX_A{X_A.shape}, X_B{X_B.shape}, X_C{X_C.shape}, X_D{X_D.shape}ã€‚"
        "è¯·æ£€æŸ¥ group/temp/cp åˆ—æ˜¯å¦ä¸ºæ•°å€¼ã€ä»¥åŠçœŸå®/é¢„æµ‹åˆ—æ˜¯å¦å­˜åœ¨ç¼ºå¤±ã€‚"
    )

# ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰=========
model_A = HuberRegressor(max_iter=10000).fit(X_A, y_A)
model_B = HuberRegressor(max_iter=10000).fit(X_B, y_B)
model_C = HuberRegressor(max_iter=10000).fit(X_C, y_C)
model_D = HuberRegressor(max_iter=10000).fit(X_D, y_D)  # æ–°å¢Dæ¨¡å‹


# ========= 6. è¯„ä¼° =========
def eval_and_print(tag, model, X, y):
    y_pred = model.predict(X)
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    ard = np.mean(np.abs((y - y_pred) / y)) * 100
    rel_err = np.abs((y_pred - y) / y) * 100
    within_1pct = int((rel_err <= 1).sum())
    within_5pct = int((rel_err <= 5).sum())
    within_10pct = int((rel_err <= 10).sum())

    print(f"\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆ{tag}ï¼‰ï¼š")
    print(f"RÂ²  = {r2:.4f}")
    print(f"MSE = {mse:.2f}")
    print(f"ARD = {ard:.2f}%")
    print(f"âœ… è¯¯å·® â‰¤ 1% : {within_1pct}")
    print(f"âœ… è¯¯å·® â‰¤ 5% : {within_5pct}")
    print(f"âœ… è¯¯å·® â‰¤ 10%: {within_10pct}")
    return y_pred


y_pred_A = eval_and_print("A=çœŸå®Î”Cp / é¢„æµ‹Î”T", model_A, X_A, y_A)
y_pred_B = eval_and_print("B=é¢„æµ‹Î”Cp / çœŸå®Î”T", model_B, X_B, y_B)
y_pred_C = eval_and_print("C=é¢„æµ‹Î”Cp / é¢„æµ‹Î”T", model_C, X_C, y_C)
y_pred_D = eval_and_print("D=çœŸå®Î”Cp / çœŸå®Î”T", model_D, X_D, y_D)  # æ–°å¢Dè¯„ä¼°

# ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
results_A = pd.DataFrame({
    "Material_ID": id_A,
    "Temperature (K)": T_A,
    "Cp_measured": y_A,
    "Cp_predicted": y_pred_A
})
results_B = pd.DataFrame({
    "Material_ID": id_B,
    "Temperature (K)": T_B,
    "Cp_measured": y_B,
    "Cp_predicted": y_pred_B
})
results_C = pd.DataFrame({
    "Material_ID": id_C,
    "Temperature (K)": T_C,
    "Cp_measured": y_C,
    "Cp_predicted": y_pred_C
})
results_D = pd.DataFrame({
    "Material_ID": id_D,
    "Temperature (K)": T_D,
    "Cp_measured": y_D,
    "Cp_predicted": y_pred_D
})

results_A.to_excel("Cpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
results_B.to_excel("Cpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
results_C.to_excel("Cpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
results_D.to_excel("Cpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_çœŸå®Î”T.xlsx", index=False)  # æ–°å¢Dç»“æœ
print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx")
print("âœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_çœŸå®Î”Cp_çœŸå®Î”T.xlsx")  # æ–°å¢Dè¾“å‡º

# ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
feature_labels = (
        list(group_cols) +  # 19 ä¸ªåŸºå›¢
        [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
        ["slopeÃ—T"]  # 1 ä¸ªæ–°ç‰¹å¾
)
coef_A = pd.DataFrame({"Feature": feature_labels, "Contribution": model_A.coef_})
coef_B = pd.DataFrame({"Feature": feature_labels, "Contribution": model_B.coef_})
coef_C = pd.DataFrame({"Feature": feature_labels, "Contribution": model_C.coef_})
coef_D = pd.DataFrame({"Feature": feature_labels, "Contribution": model_D.coef_})

coef_A.to_excel("Cpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
coef_B.to_excel("Cpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx", index=False)
coef_C.to_excel("Cpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx", index=False)
coef_D.to_excel("Cpç³»æ•°è¡¨_çœŸå®Î”Cp_çœŸå®Î”T.xlsx", index=False)
print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_çœŸå®Î”Cp_é¢„æµ‹Î”T.xlsx")
print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_çœŸå®Î”T.xlsx")
print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_é¢„æµ‹Î”Cp_é¢„æµ‹Î”T.xlsx")
print("ğŸ“ˆ å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_çœŸå®Î”Cp_çœŸå®Î”T.xlsx")