# import pandas as pd
# import numpy as np
# from sklearn.linear_model import HuberRegressor
# from sklearn.metrics import mean_squared_error, r2_score
# from sklearn.preprocessing import PolynomialFeatures
#
# # ========= 1. è¯»å–æ•°æ® =========
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ========= 2. åˆ—å®šä¹‰ =========
# group_cols = df.columns[11:30]  # 19ä¸ªåŸºå›¢åˆ—
# temp_cols = df.columns[30:40]  # 10ä¸ªæ¸©åº¦ç‚¹
# cp_cols = df.columns[40:50]  # 10ä¸ª Cp å€¼
# target_column_T1 = 'ASPEN Half Critical T'
#
# # ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
# X_groups = df[group_cols]
# valid_mask = ~df[target_column_T1].isna()
#
# # ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# # è®¡ç®—ç›¸ä¼¼åº¦æ—¶ä½¿ç”¨ log1p è½¬æ¢åçš„åŸºå›¢å‘é‡
# group_vectors_log = np.log1p(X_groups)
#
# # è®¡ç®—ç›¸ä¼¼åº¦å‡½æ•°
# def compute_msc(target_vector, reference_vector, alpha=np.e):
#     target_vector = np.array(target_vector)
#     reference_vector = np.array(reference_vector)
#     min_vals = np.minimum(target_vector, reference_vector)
#     max_vals = np.maximum(target_vector, reference_vector)
#     sum_min = np.sum(min_vals)
#     sum_max = np.sum(max_vals)
#     msc = (alpha ** sum_min - 1) / (alpha ** sum_max - 1)
#     return msc
#
#
# # æ”¹ç”¨ç›¸ä¼¼åº¦å›å½’é¢„æµ‹ T1
# y_T1 = df.loc[valid_mask, target_column_T1].values
# similarity_threshold = 0 # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
#
# # è®¡ç®—ç›®æ ‡åˆ†å­ä¸å…¶ä»–åˆ†å­çš„ç›¸ä¼¼åº¦
# T1_model = []
# for i, target_vector in enumerate(X_poly):
#     similarities_i = []
#     for j, ref_vector in enumerate(X_poly):
#         if i != j:  # æ’é™¤è‡ªèº«
#             similarity = compute_msc(group_vectors_log.iloc[i], group_vectors_log.iloc[j])  # ä½¿ç”¨å¯¹æ•°è½¬æ¢åçš„å‘é‡
#             similarities_i.append((j, similarity))
#
#     # é€‰æ‹©ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„å‚è€ƒåˆ†å­
#     selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]
#
#     # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†å­ï¼Œåˆ™é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€ä¸ª
#     if len(selected_indices) == 0:
#         max_sim_idx = max(similarities_i, key=lambda x: x[1])[0]
#         selected_indices.append(max_sim_idx)
#
#     # ä½¿ç”¨ç›¸ä¼¼åº¦è¾ƒé«˜çš„åˆ†å­è®­ç»ƒæ¨¡å‹
#     if len(selected_indices) > 0:
#         model = HuberRegressor(max_iter=9000)
#         model.fit(X_poly[selected_indices], y_T1[selected_indices])
#         T1_pred = model.predict([X_poly[i]])  # é¢„æµ‹ç›®æ ‡åˆ†å­çš„T1
#         T1_model.append(T1_pred[0])
#
# # Cp1, Cp2 ä½¿ç”¨ç›¸ä¼¼åº¦å›å½’æ¨¡å‹
# Cp1_model = []
# Cp2_model = []
# for i, target_vector in enumerate(X_groups[valid_mask]):
#     similarities_i = []
#     for j, ref_vector in enumerate(X_groups[valid_mask]):
#         if i != j:  # æ’é™¤è‡ªèº«
#             similarity = compute_msc(group_vectors_log.iloc[i], group_vectors_log.iloc[j])  # ä½¿ç”¨å¯¹æ•°è½¬æ¢åçš„å‘é‡
#             similarities_i.append((j, similarity))
#
#     # é€‰æ‹©ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„å‚è€ƒåˆ†å­
#     selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]
#
#     # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†å­ï¼Œåˆ™é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€ä¸ª
#     if len(selected_indices) == 0:
#         max_sim_idx = max(similarities_i, key=lambda x: x[1])[0]
#         selected_indices.append(max_sim_idx)
#
#     if len(selected_indices) > 0:
#         model1 = HuberRegressor(max_iter=9000)
#         model1.fit(X_groups.iloc[selected_indices], df.loc[selected_indices, df.columns[9]])  # ç”¨Cp1çš„æ•°æ®è®­ç»ƒ
#         Cp1_pred = model1.predict([X_groups.iloc[i]])  # é¢„æµ‹ç›®æ ‡åˆ†å­çš„Cp1
#         Cp1_model.append(Cp1_pred[0])
#
#         model2 = HuberRegressor(max_iter=9000)
#         model2.fit(X_groups.iloc[selected_indices], df.loc[selected_indices, df.columns[50]])  # ç”¨Cp2çš„æ•°æ®è®­ç»ƒ
#         Cp2_pred = model2.predict([X_groups.iloc[i]])  # é¢„æµ‹ç›®æ ‡åˆ†å­çš„Cp2
#         Cp2_model.append(Cp2_pred[0])
#
# # ========= 3.1 å­æ¨¡å‹è¯„ä¼° =========
# y_pred_T1 = np.array(T1_model)
# r2_T1 = r2_score(y_T1, y_pred_T1)
# mse_T1 = mean_squared_error(y_T1, y_pred_T1)
#
# y_Cp1_true = df.iloc[:, 9]
# y_Cp1_pred = np.array(Cp1_model)
# r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred)
# mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred)
#
# y_Cp2_true = df.iloc[:, 50]
# y_Cp2_pred = np.array(Cp2_model)
# r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred)
# mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred)
#
# print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
# print(f"T1_model ->     RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
# print(f"Cp1_model ->    RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
# print(f"Cp2_model ->    RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")
#
# # ========= 4. æ„å»ºè®­ç»ƒæ•°æ® =========
# X_total, y_total, material_ids, temperatures = [], [], [], []
# X_poly_all = poly.transform(X_groups)
#
# for i, row in df.iterrows():
#     material_id = row.iloc[0]
#     Nk = row[group_cols].values
#     temps = row[temp_cols].values
#     cps = row[cp_cols].values
#
#     Nk_df = pd.DataFrame([Nk], columns=group_cols)
#     Nk_poly = X_poly_all[i:i + 1]
#
#     try:
#         # æ–°æ¨¡å‹ï¼šç›´æ¥é¢„æµ‹ T1ï¼ˆæ— éœ€ log å’Œ expï¼‰
#         T1 = T1_model[i]
#         if T1 <= 0 or np.isnan(T1):
#             continue
#         T2 = T1 * 1.5
#         Cp1 = Cp1_model[i]
#         Cp2 = Cp2_model[i]
#         slope = (Cp2 - Cp1) / (T2 - T1)
#     except:
#         continue
#
#     for T, Cp in zip(temps, cps):
#         if np.isnan(T) or np.isnan(Cp):
#             continue
#
#         features = np.concatenate([
#             Nk,  # 19 ä¸ªåŸºå›¢
#             Nk * T,  # 19 ä¸ªäº¤äº’é¡¹
#             [slope * T]  # slope Ã— T
#         ])
#
#         X_total.append(features)
#         y_total.append(Cp)
#         material_ids.append(material_id)
#         temperatures.append(T)
#
# # ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰ =========
# X_total = np.array(X_total)
# y_total = np.array(y_total)
#
# model = HuberRegressor(max_iter=10000).fit(X_total, y_total)
#
# # ========= 6. æ¨¡å‹è¯„ä¼° =========
# y_pred = model.predict(X_total)
# mse = mean_squared_error(y_total, y_pred)
# r2 = r2_score(y_total, y_pred)
# ard = np.mean(np.abs((y_total - y_pred) / y_total)) * 100
#
# # === æ–°å¢è¯¯å·®ç»Ÿè®¡ ===
# relative_error = np.abs((y_pred - y_total) / y_total) * 100
# within_1pct = np.sum(relative_error <= 1)
# within_5pct = np.sum(relative_error <= 5)
# within_10pct = np.sum(relative_error <= 10)
#
# print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆå« slopeÃ—T ç‰¹å¾ï¼‰ï¼š")
# print(f"RÂ²  = {r2:.4f}")
# print(f"MSE = {mse:.2f}")
# print(f"ARD = {ard:.2f}%")
# print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct}")
# print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct}")
# print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct}")
#
# # ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
# results = pd.DataFrame({
#     "Material_ID": material_ids,
#     "Temperature (K)": temperatures,
#     "Cp_measured": y_total,
#     "Cp_predicted": y_pred
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
# print("âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœä¸º: Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")
#
# # ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
# feature_labels = (
#         list(group_cols) +  # 19 ä¸ªåŸºå›¢
#         [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
#         ["slopeÃ—T"]  # 1 ä¸ªæ–°ç‰¹å¾
# )
#
# coefficients = pd.DataFrame({
#     "Feature": feature_labels,
#     "Contribution": model.coef_
# })
# coefficients.to_excel("Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
# print("ğŸ“ˆ å·²ä¿å­˜æ¨¡å‹ç³»æ•°ä¸º: Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")
#
# import pandas as pd
# import numpy as np
# from sklearn.linear_model import HuberRegressor
# from sklearn.metrics import mean_squared_error, r2_score
#
# # ========= 1. è¯»å–æ•°æ® =========
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ========= 2. åˆ—å®šä¹‰ =========
# group_cols = df.columns[11:30]  # 19ä¸ªåŸºå›¢åˆ—
# temp_cols = df.columns[30:40]  # 10ä¸ªæ¸©åº¦ç‚¹
# cp_cols = df.columns[40:50]  # 10ä¸ª Cp å€¼
# target_column_T1 = 'ASPEN Half Critical T'
#
# # ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
# X_groups = df[group_cols]  # ä½¿ç”¨åŸå§‹åŸºå›¢å‘é‡
# valid_mask = ~df[target_column_T1].isna()
#
# # è®¡ç®—ç›¸ä¼¼åº¦æ—¶ä½¿ç”¨ log1p è½¬æ¢åçš„åŸºå›¢å‘é‡
# group_vectors_log = np.log1p(X_groups)
#
# # è®¡ç®—ç›¸ä¼¼åº¦å‡½æ•°
# def compute_msc(target_vector, reference_vector, alpha=np.e):
#     target_vector = np.array(target_vector)
#     reference_vector = np.array(reference_vector)
#     min_vals = np.minimum(target_vector, reference_vector)
#     max_vals = np.maximum(target_vector, reference_vector)
#     sum_min = np.sum(min_vals)
#     sum_max = np.sum(max_vals)
#     msc = (alpha ** sum_min - 1) / (alpha ** sum_max - 1)
#     return msc
#
#
# # æ”¹ç”¨ç›¸ä¼¼åº¦å›å½’é¢„æµ‹ T1
# y_T1 = df.loc[valid_mask, target_column_T1].values
# similarity_threshold = 0  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
#
# print("å¼€å§‹è®­ç»ƒT1æ¨¡å‹...")
#
# T1_predictions = np.full(len(df), np.nan)  # ä¸ºæ‰€æœ‰åˆ†å­åˆ›å»ºæ•°ç»„ï¼Œåˆå§‹ä¸ºNaN
#
# # è·å–æœ‰æœ‰æ•ˆT1å€¼çš„åˆ†å­ç´¢å¼•
# valid_indices = valid_mask[valid_mask].index.tolist()
#
# # åªä¸ºæœ‰æœ‰æ•ˆT1å€¼çš„åˆ†å­é¢„æµ‹T1ï¼Œä½†ä»æ‰€æœ‰åˆ†å­ä¸­é€‰æ‹©ç›¸ä¼¼åˆ†å­
# for i, orig_i_idx in enumerate(valid_indices):
#     if i % 10 == 0:
#         print(f"å¤„ç†T1ç¬¬ {i}/{len(valid_indices)} ä¸ªåˆ†å­...")
#
#     similarities_i = []
#     # ä»æ‰€æœ‰åˆ†å­ä¸­é€‰æ‹©ç›¸ä¼¼åˆ†å­ï¼ˆä¸ä»…ä»…æ˜¯19ä¸ªæœ‰T1å€¼çš„ï¼‰
#     for j in range(len(df)):
#         if j != orig_i_idx:  # æ’é™¤è‡ªèº«
#             similarity = compute_msc(group_vectors_log.iloc[orig_i_idx], group_vectors_log.iloc[j])
#             similarities_i.append((j, similarity))
#
#     # é€‰æ‹©ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„å‚è€ƒåˆ†å­ï¼ˆä»æ‰€æœ‰åˆ†å­ä¸­é€‰æ‹©ï¼‰
#     selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]
#
#     # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†å­ï¼Œåˆ™é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€ä¸ª
#     if len(selected_indices) == 0 and len(similarities_i) > 0:
#         selected_indices.append(max(similarities_i, key=lambda x: x[1])[0])
#
#     # åªä½¿ç”¨æœ‰T1å€¼çš„ç›¸ä¼¼åˆ†å­è¿›è¡Œè®­ç»ƒ
#     valid_selected_indices = [idx for idx in selected_indices if valid_mask[idx]]
#
#     if len(valid_selected_indices) > 0:
#         # è·å–è¿™äº›åˆ†å­åœ¨åŸå§‹åŸºå›¢å‘é‡ä¸­çš„ç´¢å¼•
#         poly_indices = [valid_indices.index(idx) for idx in valid_selected_indices if idx in valid_indices]
#
#         if len(poly_indices) > 0:
#             model = HuberRegressor(max_iter=9000000)
#             model.fit(X_groups.iloc[poly_indices], y_T1[poly_indices])
#             T1_pred = model.predict([X_groups.iloc[orig_i_idx]])[0]
#             T1_predictions[orig_i_idx] = T1_pred
#     else:
#         # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å¹³å‡å€¼
#         T1_predictions[orig_i_idx] = np.mean(y_T1)
#
# print("å¼€å§‹è®­ç»ƒCp1å’ŒCp2æ¨¡å‹...")
# Cp1_predictions = np.full(len(df), np.nan)
# Cp2_predictions = np.full(len(df), np.nan)
#
# # ä¸ºæ‰€æœ‰åˆ†å­é¢„æµ‹Cp1å’ŒCp2
# for i in range(len(df)):
#     if i % 10 == 0:
#         print(f"å¤„ç†Cpç¬¬ {i}/{len(df)} ä¸ªåˆ†å­...")
#
#     similarities_i = []
#     for j in range(len(df)):
#         if i != j:  # æ’é™¤è‡ªèº«
#             similarity = compute_msc(group_vectors_log.iloc[i], group_vectors_log.iloc[j])
#             similarities_i.append((j, similarity))
#
#     # é€‰æ‹©ç›¸ä¼¼åˆ†å­
#     selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]
#     if len(selected_indices) == 0 and len(similarities_i) > 0:
#         selected_indices.append(max(similarities_i, key=lambda x: x[1])[0])
#
#     if len(selected_indices) > 0:
#         # è½¬æ¢ä¸ºnumpyæ•°ç»„é¿å…ç‰¹å¾åç§°è­¦å‘Š
#         X_array = X_groups.values
#
#         # è®­ç»ƒCp1æ¨¡å‹
#         model1 = HuberRegressor(max_iter=9000000)
#         model1.fit(X_array[selected_indices], df.iloc[selected_indices, 9])
#         Cp1_pred = model1.predict([X_array[i]])[0]
#         Cp1_predictions[i] = Cp1_pred
#
#         # è®­ç»ƒCp2æ¨¡å‹
#         model2 = HuberRegressor(max_iter=9000000)
#         model2.fit(X_array[selected_indices], df.iloc[selected_indices, 50])
#         Cp2_pred = model2.predict([X_array[i]])[0]
#         Cp2_predictions[i] = Cp2_pred
#     else:
#         # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å¹³å‡å€¼
#         Cp1_predictions[i] = np.nanmean(df.iloc[:, 9])
#         Cp2_predictions[i] = np.nanmean(df.iloc[:, 50])
#
# # ========= 3.1 å­æ¨¡å‹è¯„ä¼° =========
# # åªè¯„ä¼°æœ‰æœ‰æ•ˆå€¼çš„éƒ¨åˆ†
# valid_T1_mask = ~np.isnan(T1_predictions) & valid_mask
# y_pred_T1 = T1_predictions[valid_T1_mask]
# y_true_T1 = df.loc[valid_T1_mask, target_column_T1].values
#
# r2_T1 = r2_score(y_true_T1, y_pred_T1) if len(y_true_T1) > 0 else np.nan
# mse_T1 = mean_squared_error(y_true_T1, y_pred_T1) if len(y_true_T1) > 0 else np.nan
#
# # Cp1å’ŒCp2è¯„ä¼°æ‰€æœ‰åˆ†å­
# valid_Cp_mask = ~np.isnan(Cp1_predictions) & ~np.isnan(df.iloc[:, 9])
# y_Cp1_true = df.iloc[valid_Cp_mask, 9].values
# y_Cp1_pred = Cp1_predictions[valid_Cp_mask]
# r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred) if len(y_Cp1_true) > 0 else np.nan
# mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred) if len(y_Cp1_true) > 0 else np.nan
#
# valid_Cp2_mask = ~np.isnan(Cp2_predictions) & ~np.isnan(df.iloc[:, 50])
# y_Cp2_true = df.iloc[valid_Cp2_mask, 50].values
# y_Cp2_pred = Cp2_predictions[valid_Cp2_mask]
# r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred) if len(y_Cp2_true) > 0 else np.nan
# mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred) if len(y_Cp2_true) > 0 else np.nan
#
# print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
# print(f"T1_model ->     æ ·æœ¬æ•°: {len(y_true_T1)}, RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
# print(f"Cp1_model ->    æ ·æœ¬æ•°: {len(y_Cp1_true)}, RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
# print(f"Cp2_model ->    æ ·æœ¬æ•°: {len(y_Cp2_true)}, RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")
#
# # ========= 4. æ„å»ºè®­ç»ƒæ•°æ® =========
# X_total, y_total, material_ids, temperatures = [], [], [], []
#
# for i, row in df.iterrows():
#     material_id = row.iloc[0]
#     Nk = row[group_cols].values
#     temps = row[temp_cols].values
#     cps = row[cp_cols].values
#
#     # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹å€¼
#     if np.isnan(T1_predictions[i]) or np.isnan(Cp1_predictions[i]) or np.isnan(Cp2_predictions[i]):
#         continue
#
#     try:
#         T1 = T1_predictions[i]
#         if T1 <= 0:
#             continue
#         T2 = T1 * 1.5
#         Cp1 = Cp1_predictions[i]
#         Cp2 = Cp2_predictions[i]
#         slope = (Cp2 - Cp1) / (T2 - T1) if (T2 - T1) != 0 else 0
#     except:
#         continue
#
#     for T, Cp in zip(temps, cps):
#         if np.isnan(T) or np.isnan(Cp):
#             continue
#
#         features = np.concatenate([Nk, Nk * T, [slope * T]])
#
#         X_total.append(features)
#         y_total.append(Cp)
#         material_ids.append(material_id)
#         temperatures.append(T)
#
# # ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰ =========
# if len(X_total) > 0:
#     X_total = np.array(X_total)
#     y_total = np.array(y_total)
#
#     model = HuberRegressor(max_iter=10000).fit(X_total, y_total)
#
#     # ========= 6. æ¨¡å‹è¯„ä¼° =========
#     y_pred = model.predict(X_total)
#     mse = mean_squared_error(y_total, y_pred)
#     r2 = r2_score(y_total, y_pred)
#     ard = np.mean(np.abs((y_total - y_pred) / y_total)) * 100
#
#     # === æ–°å¢è¯¯å·®ç»Ÿè®¡ ===
#     relative_error = np.abs((y_pred - y_total) / y_total) * 100
#     within_1pct = np.sum(relative_error <= 1)
#     within_5pct = np.sum(relative_error <= 5)
#     within_10pct = np.sum(relative_error <= 10)
#
#     print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆå« slopeÃ—T ç‰¹å¾ï¼‰ï¼š")
#     print(f"RÂ²  = {r2:.4f}")
#     print(f"MSE = {mse:.2f}")
#     print(f"ARD = {ard:.2f}%")
#     print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct}")
#     print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct}")
#
#     # ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
#     results = pd.DataFrame({
#         "Material_ID": material_ids,
#         "Temperature (K)": temperatures,
#         "Cp_measured": y_total,
#         "Cp_predicted": y_pred
#     })
#     results.to_excel("Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
#     print("âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœä¸º: Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")
#
#     # ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
#     feature_labels = (
#             list(group_cols) +  # 19 ä¸ªåŸºå›¢
#             [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
#             ["slopeÃ—T"]  # 1 ä¸ªæ–°ç‰¹å¾
#     )
#
#     coefficients = pd.DataFrame({
#         "Feature": feature_labels,
#         "Contribution": model.coef_
#     })
#     coefficients.to_excel("Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
#     print("ğŸ“ˆ å·²ä¿å­˜æ¨¡å‹ç³»æ•°ä¸º: Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")
# else:
#     print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ç”¨äºè®­ç»ƒæ€»æ¨¡å‹")

import pandas as pd
import numpy as np
from sklearn.linear_model import HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# ========= 1. è¯»å–æ•°æ® =========
file_path = "heat capacity 207.xlsx"
df = pd.read_excel(file_path, sheet_name="Sheet1")
df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ========= 2. åˆ—å®šä¹‰ =========
group_cols = df.columns[11:30]  # 19ä¸ªåŸºå›¢åˆ—
temp_cols = df.columns[30:40]  # 10ä¸ªæ¸©åº¦ç‚¹
cp_cols = df.columns[40:50]  # 10ä¸ª Cp å€¼
target_column_T1 = 'ASPEN Half Critical T'

# ========= 3. å­æ¨¡å‹è®­ç»ƒ =========
X_groups = df[group_cols]  # ä½¿ç”¨åŸå§‹åŸºå›¢å‘é‡
valid_mask = ~df[target_column_T1].isna()

# è®¡ç®—ç›¸ä¼¼åº¦æ—¶ä½¿ç”¨ log1p è½¬æ¢åçš„åŸºå›¢å‘é‡
group_vectors_log = np.log1p(X_groups)


# è®¡ç®—ç›¸ä¼¼åº¦å‡½æ•°
def compute_msc(target_vector, reference_vector, alpha=np.e):
    target_vector = np.array(target_vector)
    reference_vector = np.array(reference_vector)
    min_vals = np.minimum(target_vector, reference_vector)
    max_vals = np.maximum(target_vector, reference_vector)
    sum_min = np.sum(min_vals)
    sum_max = np.sum(max_vals)
    if sum_max == 0:  # æ·»åŠ é™¤ä»¥é›¶ä¿æŠ¤
        return 0
    msc = (alpha ** sum_min - 1) / (alpha ** sum_max - 1)
    return msc


# æ”¹ç”¨ç›¸ä¼¼åº¦å›å½’é¢„æµ‹ T1
y_T1 = df.loc[valid_mask, target_column_T1].values
similarity_threshold = 0.9  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè·å¾—æ›´å¤šæ ·æœ¬

print("å¼€å§‹è®­ç»ƒT1æ¨¡å‹...")
T1_predictions = np.full(len(df), np.nan)  # ä¸ºæ‰€æœ‰åˆ†å­åˆ›å»ºæ•°ç»„ï¼Œåˆå§‹ä¸ºNaN

# è·å–æœ‰æœ‰æ•ˆT1å€¼çš„åˆ†å­ç´¢å¼•
valid_indices = valid_mask[valid_mask].index.tolist()

# åªä¸ºæœ‰æœ‰æ•ˆT1å€¼çš„åˆ†å­é¢„æµ‹T1ï¼Œä½†ä»æ‰€æœ‰åˆ†å­ä¸­é€‰æ‹©ç›¸ä¼¼åˆ†å­
for i, orig_i_idx in enumerate(valid_indices):
    if i % 10 == 0:
        print(f"å¤„ç†T1ç¬¬ {i}/{len(valid_indices)} ä¸ªåˆ†å­...")

    similarities_i = []
    # ä»æ‰€æœ‰åˆ†å­ä¸­é€‰æ‹©ç›¸ä¼¼åˆ†å­
    for j in range(len(df)):
        if j != orig_i_idx:  # æ’é™¤è‡ªèº«
            similarity = compute_msc(group_vectors_log.iloc[orig_i_idx], group_vectors_log.iloc[j])
            similarities_i.append((j, similarity))

    # é€‰æ‹©ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„å‚è€ƒåˆ†å­
    selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]

    # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†å­ï¼Œåˆ™é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€ä¸ª
    if len(selected_indices) == 0 and len(similarities_i) > 0:
        selected_indices.append(max(similarities_i, key=lambda x: x[1])[0])

    # åªä½¿ç”¨æœ‰T1å€¼çš„ç›¸ä¼¼åˆ†å­è¿›è¡Œè®­ç»ƒ
    valid_selected_indices = [idx for idx in selected_indices if valid_mask[idx]]

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€å®šæ•°é‡çš„è®­ç»ƒæ ·æœ¬
    if len(valid_selected_indices) < 5 and len(valid_indices) > 5:
        # ä»æ‰€æœ‰æœ‰T1å€¼çš„åˆ†å­ä¸­è¡¥å……ä¸€äº›
        additional_indices = [idx for idx in valid_indices if idx != orig_i_idx and idx not in valid_selected_indices]
        if len(additional_indices) > 0:
            valid_selected_indices.extend(additional_indices[:min(5, len(additional_indices))])

    if len(valid_selected_indices) > 0:
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train = X_groups.iloc[valid_selected_indices].values
            X_test = X_groups.iloc[orig_i_idx:orig_i_idx + 1].values

            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # ä½¿ç”¨åˆç†çš„Huberå›å½’å‚æ•°
            model = HuberRegressor(max_iter=10000, epsilon=1.5, alpha=0.0001)
            y_train = df.loc[valid_selected_indices, target_column_T1].values

            model.fit(X_train_scaled, y_train)
            T1_pred = model.predict(X_test_scaled)[0]
            T1_predictions[orig_i_idx] = T1_pred

        except Exception as e:
            print(f"åˆ†å­ {orig_i_idx} è®­ç»ƒå¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç›¸ä¼¼åˆ†å­çš„å¹³å‡å€¼
            T1_predictions[orig_i_idx] = np.mean(df.loc[valid_selected_indices, target_column_T1]) if len(
                valid_selected_indices) > 0 else np.mean(y_T1)
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å¹³å‡å€¼
        T1_predictions[orig_i_idx] = np.mean(y_T1)

print("å¼€å§‹è®­ç»ƒCp1å’ŒCp2æ¨¡å‹...")
Cp1_predictions = np.full(len(df), np.nan)
Cp2_predictions = np.full(len(df), np.nan)

# ä¸ºæ‰€æœ‰åˆ†å­é¢„æµ‹Cp1å’ŒCp2
for i in range(len(df)):
    if i % 10 == 0:
        print(f"å¤„ç†Cpç¬¬ {i}/{len(df)} ä¸ªåˆ†å­...")

    similarities_i = []
    for j in range(len(df)):
        if i != j:  # æ’é™¤è‡ªèº«
            similarity = compute_msc(group_vectors_log.iloc[i], group_vectors_log.iloc[j])
            similarities_i.append((j, similarity))

    # é€‰æ‹©ç›¸ä¼¼åˆ†å­
    selected_indices = [j for j, similarity in similarities_i if similarity > similarity_threshold]
    if len(selected_indices) == 0 and len(similarities_i) > 0:
        selected_indices.append(max(similarities_i, key=lambda x: x[1])[0])

    if len(selected_indices) > 0:
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train = X_groups.iloc[selected_indices].values
            X_test = X_groups.iloc[i:i + 1].values

            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # è®­ç»ƒCp1æ¨¡å‹
            model1 = HuberRegressor(max_iter=10000, epsilon=1.5, alpha=0.0001)
            y_train_cp1 = df.iloc[selected_indices, 9].values
            model1.fit(X_train_scaled, y_train_cp1)
            Cp1_pred = model1.predict(X_test_scaled)[0]
            Cp1_predictions[i] = Cp1_pred

            # è®­ç»ƒCp2æ¨¡å‹
            model2 = HuberRegressor(max_iter=10000, epsilon=1.5, alpha=0.0001)
            y_train_cp2 = df.iloc[selected_indices, 50].values
            model2.fit(X_train_scaled, y_train_cp2)
            Cp2_pred = model2.predict(X_test_scaled)[0]
            Cp2_predictions[i] = Cp2_pred

        except Exception as e:
            print(f"åˆ†å­ {i} çš„Cpé¢„æµ‹å¤±è´¥: {e}")
            Cp1_predictions[i] = np.nanmean(df.iloc[:, 9])
            Cp2_predictions[i] = np.nanmean(df.iloc[:, 50])
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å¹³å‡å€¼
        Cp1_predictions[i] = np.nanmean(df.iloc[:, 9])
        Cp2_predictions[i] = np.nanmean(df.iloc[:, 50])

# ========= 3.1 å­æ¨¡å‹è¯„ä¼° =========
# åªè¯„ä¼°æœ‰æœ‰æ•ˆå€¼çš„éƒ¨åˆ†
valid_T1_mask = ~np.isnan(T1_predictions) & valid_mask
y_pred_T1 = T1_predictions[valid_T1_mask]
y_true_T1 = df.loc[valid_T1_mask, target_column_T1].values

r2_T1 = r2_score(y_true_T1, y_pred_T1) if len(y_true_T1) > 0 else np.nan
mse_T1 = mean_squared_error(y_true_T1, y_pred_T1) if len(y_true_T1) > 0 else np.nan

# Cp1å’ŒCp2è¯„ä¼°æ‰€æœ‰åˆ†å­ - ä¿®å¤ç´¢å¼•é”™è¯¯
# æ–¹æ³•1ï¼šä½¿ç”¨ numpy æ•°ç»„è¿›è¡Œå¸ƒå°”ç´¢å¼•
cp1_valid_mask = ~np.isnan(Cp1_predictions) & ~np.isnan(df.iloc[:, 9].values)
y_Cp1_true = df.iloc[:, 9].values[cp1_valid_mask]
y_Cp1_pred = Cp1_predictions[cp1_valid_mask]

cp2_valid_mask = ~np.isnan(Cp2_predictions) & ~np.isnan(df.iloc[:, 50].values)
y_Cp2_true = df.iloc[:, 50].values[cp2_valid_mask]
y_Cp2_pred = Cp2_predictions[cp2_valid_mask]

r2_Cp1 = r2_score(y_Cp1_true, y_Cp1_pred) if len(y_Cp1_true) > 0 else np.nan
mse_Cp1 = mean_squared_error(y_Cp1_true, y_Cp1_pred) if len(y_Cp1_true) > 0 else np.nan

r2_Cp2 = r2_score(y_Cp2_true, y_Cp2_pred) if len(y_Cp2_true) > 0 else np.nan
mse_Cp2 = mean_squared_error(y_Cp2_true, y_Cp2_pred) if len(y_Cp2_true) > 0 else np.nan

print("\nğŸ“Œ å­æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
print(f"T1_model ->     æ ·æœ¬æ•°: {len(y_true_T1)}, RÂ²: {r2_T1:.4f} | MSE: {mse_T1:.4f}")
print(f"Cp1_model ->    æ ·æœ¬æ•°: {len(y_Cp1_true)}, RÂ²: {r2_Cp1:.4f} | MSE: {mse_Cp1:.4f}")
print(f"Cp2_model ->    æ ·æœ¬æ•°: {len(y_Cp2_true)}, RÂ²: {r2_Cp2:.4f} | MSE: {mse_Cp2:.4f}")

# ========= 4. æ„å»ºè®­ç»ƒæ•°æ® =========
X_total, y_total, material_ids, temperatures = [], [], [], []

for i, row in df.iterrows():
    material_id = row.iloc[0]
    Nk = row[group_cols].values
    temps = row[temp_cols].values
    cps = row[cp_cols].values

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹å€¼
    if np.isnan(T1_predictions[i]) or np.isnan(Cp1_predictions[i]) or np.isnan(Cp2_predictions[i]):
        continue

    try:
        T1 = T1_predictions[i]
        if T1 <= 0:
            continue
        T2 = T1 * 1.5
        Cp1 = Cp1_predictions[i]
        Cp2 = Cp2_predictions[i]
        slope = (Cp2 - Cp1) / (T2 - T1) if (T2 - T1) != 0 else 0
    except:
        continue

    for T, Cp in zip(temps, cps):
        if np.isnan(T) or np.isnan(Cp):
            continue

        features = np.concatenate([Nk, Nk * T, [slope * T]])

        X_total.append(features)
        y_total.append(Cp)
        material_ids.append(material_id)
        temperatures.append(T)

# ========= 5. æ¨¡å‹æ‹Ÿåˆï¼ˆHuberï¼‰ =========
if len(X_total) > 0:
    X_total = np.array(X_total)
    y_total = np.array(y_total)

    # å¯¹æ€»æ¨¡å‹ç‰¹å¾ä¹Ÿè¿›è¡Œæ ‡å‡†åŒ–
    scaler_total = StandardScaler()
    X_total_scaled = scaler_total.fit_transform(X_total)

    model = HuberRegressor(max_iter=10000, epsilon=1.5, alpha=0.0001).fit(X_total_scaled, y_total)

    # ========= 6. æ¨¡å‹è¯„ä¼° =========
    y_pred = model.predict(X_total_scaled)
    mse = mean_squared_error(y_total, y_pred)
    r2 = r2_score(y_total, y_pred)
    ard = np.mean(np.abs((y_total - y_pred) / y_total)) * 100

    # === æ–°å¢è¯¯å·®ç»Ÿè®¡ ===
    relative_error = np.abs((y_pred - y_total) / y_total) * 100
    within_1pct = np.sum(relative_error <= 1)
    within_5pct = np.sum(relative_error <= 5)
    within_10pct = np.sum(relative_error <= 10)

    print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆå« slopeÃ—T ç‰¹å¾ï¼‰ï¼š")
    print(f"RÂ²  = {r2:.4f}")
    print(f"MSE = {mse:.2f}")
    print(f"ARD = {ard:.2f}%")
    print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct}")
    print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct}")
    print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct}")

    # ========= 7. è¾“å‡ºé¢„æµ‹ç»“æœ =========
    results = pd.DataFrame({
        "Material_ID": material_ids,
        "Temperature (K)": temperatures,
        "Cp_measured": y_total,
        "Cp_predicted": y_pred
    })
    results.to_excel("Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
    print("âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœä¸º: Cpé¢„æµ‹ç»“æœ_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")

    # ========= 8. è¾“å‡ºç³»æ•°è¡¨ =========
    feature_labels = (
            list(group_cols) +  # 19 ä¸ªåŸºå›¢
            [f"{g}_T" for g in group_cols] +  # 19 ä¸ªåŸºå›¢ Ã— T
            ["slopeÃ—T"]  # 1 ä¸ªæ–°ç‰¹å¾
    )

    coefficients = pd.DataFrame({
        "Feature": feature_labels,
        "Contribution": model.coef_
    })
    coefficients.to_excel("Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx", index=False)
    print("ğŸ“ˆ å·²ä¿å­˜æ¨¡å‹ç³»æ•°ä¸º: Cpç³»æ•°è¡¨_slopeTç‰¹å¾_ç›¸ä¼¼åº¦å›å½’.xlsx")
else:
    print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ç”¨äºè®­ç»ƒæ€»æ¨¡å‹")
