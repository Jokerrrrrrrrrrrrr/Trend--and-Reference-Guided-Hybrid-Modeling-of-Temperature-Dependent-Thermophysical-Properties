
#slow
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. Huber æŸå¤±å‡½æ•° =====
# def huber_loss(y_true, y_pred, delta=1.0):
#     """è®¡ç®— Huber æŸå¤±"""
#     error = np.abs(y_true - y_pred)
#     loss = np.where(error <= delta, 0.5 * error**2, delta * (error - 0.5 * delta))
#     return np.sum(loss)
#
#
# # ===== 8. ä¿®æ­£ï¼šåŸºäºåæ¯”ä¾‹çš„è‡ªé€‚åº”æƒé‡è®¡ç®— =====
# def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
#     """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶ä½¿ç”¨åæ¯”ä¾‹è°ƒæ•´æƒé‡"""
#     L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡çº§
#     L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
#                      np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
#     L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§
#
#     L_exp_typical = max(L_exp_typical, 1e-10)
#     L_ref_typical = max(L_ref_typical, 1e-10)
#     L_slope_typical = max(L_slope_typical, 1e-10)
#
#     print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")
#
#     # ä½¿ç”¨åæ¯”ä¾‹å…³ç³»ï¼šæ•°å€¼è¶Šå°ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ”¾å¤§ä½œç”¨ï¼‰
#     w1_base = 1.0/ L_exp_typical  # å®éªŒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w2_base = 1.0 / L_ref_typical  # å‚è€ƒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w3_base = 1.0 / L_slope_typical  # æ–œç‡æ•°å€¼å°ï¼Œæƒé‡å¤§ï¼ˆæ”¾å¤§ï¼ï¼‰
#
#     # å½’ä¸€åŒ–
#     total_base = w1_base + w2_base + w3_base
#     w1_normalized = w1_base / total_base
#     w2_normalized = w2_base / total_base
#     w3_normalized = w3_base / total_base
#
#     return w1_normalized, w2_normalized, w3_normalized
#
#
# # ===== 8.1 è®¡ç®—æŸå¤±å€æ•°ï¼ˆç”¨äºæŸå¤±å‡½æ•°å†…éƒ¨æ”¾å¤§ï¼‰ =====
# def calculate_loss_multipliers(L_exp, L_ref, L_slope, max_multiplier=1000):
#     """è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°"""
#     max_loss = max(L_exp, L_ref, L_slope)
#
#     # è®¡ç®—éœ€è¦æ”¾å¤§çš„å€æ•°
#     multiplier_exp = max_loss / L_exp
#     multiplier_ref = max_loss / L_ref
#     multiplier_slope = max_loss / L_slope
#
#     # é™åˆ¶æœ€å¤§å€æ•°ï¼Œé¿å…æç«¯å€¼
#     multiplier_exp = min(multiplier_exp, max_multiplier)
#     multiplier_ref = min(multiplier_ref, max_multiplier)
#     multiplier_slope = min(multiplier_slope, max_multiplier)
#
#     return multiplier_exp, multiplier_ref, multiplier_slope
#
#
# # ===== 7.1 ä¿®æ­£ï¼šä½¿ç”¨æ”¾å¤§å€æ•°çš„æŸå¤±å‡½æ•° =====
# def loss_sum_three_parts_with_multipliers(y_exp_true, y_exp_pred,
#                                           Cp1_true, Cp2_true,
#                                           Cp1_pred, Cp2_pred,
#                                           slope_true, slope_pred,
#                                           w1, w2, w3):
#     """ä½¿ç”¨æ”¾å¤§å€æ•°åçš„æŸå¤±å‡½æ•°"""
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred)) * multiplier_exp
#     L_ref = (np.sum(np.abs(Cp1_true - Cp1_pred)) +
#              np.sum(np.abs(Cp2_true - Cp2_pred))) * multiplier_ref
#     L_slope = np.sum(np.abs(slope_true - slope_pred)) * multiplier_slope
#
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # è®¡ç®—åæ¯”ä¾‹åŸºå‡†æƒé‡
# base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
#     X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
# )
#
# # è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°
# L_exp, L_ref, L_slope = (
#     np.mean(np.abs(y_exp - np.mean(y_exp))),
#     (np.mean(np.abs(Cp1_true_all - np.mean(Cp1_true_all))) +
#      np.mean(np.abs(Cp2_true_all - np.mean(Cp2_true_all)))) / 2,
#     np.mean(np.abs(slope_true_all - np.mean(slope_true_all)))
# )
# multiplier_exp, multiplier_ref, multiplier_slope = calculate_loss_multipliers(L_exp, L_ref, L_slope)
#
#
# # ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
# candidate_ws = sample_weight_triplets(n=10, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
#
# # ===== 9.1 å†…å±‚ä¼˜åŒ–å‡½æ•°å®šä¹‰ =====
# def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                            Cp1_true_train, Cp2_true_train, slope_true_train,
#                            T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0
#
#     def objective(theta):
#         beta = theta[:-1]
#         b = theta[-1]
#
#         # å®éªŒç‚¹é¢„æµ‹
#         y_pred_train = X_exp_train @ beta + b
#
#         # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
#         unique_materials = np.unique(mat_idx_train)
#         Cp1_pred_train = np.zeros(len(unique_materials))
#         Cp2_pred_train = np.zeros(len(unique_materials))
#
#         for i, mat_idx in enumerate(unique_materials):
#             Nk = X_groups.iloc[mat_idx].values.astype(float)
#             s_feat = slope_feat_all[mat_idx]
#
#             x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#             x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#             Cp1_pred_train[i] = x_T1 @ beta + b
#             Cp2_pred_train[i] = x_T2 @ beta + b
#
#         # æ–œç‡é¢„æµ‹
#         slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
#                 T2_hat_all[unique_materials] - T1_hat_all[unique_materials])
#
#         # ä½¿ç”¨å¸¦æ”¾å¤§å€æ•°çš„æŸå¤±å‡½æ•°
#         return loss_sum_three_parts_with_multipliers(
#             y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#             Cp1_true=Cp1_true_train[unique_materials],
#             Cp2_true=Cp2_true_train[unique_materials],
#             Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#             slope_true=slope_true_train[unique_materials],
#             slope_pred=slope_pred_train,
#             w1=w1, w2=w2, w3=w3
#         )
#
#     res = minimize(objective, theta0, method="Powell", options={"maxiter": 5000, "xtol": 1e-6, "ftol": 1e-6})
#     return res
#
#
# # ===== 9.2 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
# for i, w in enumerate(candidate_ws):
#     # ä½¿ç”¨å¹‚å‡½æ•°è¿›ä¸€æ­¥æ”¾å¤§æ–œç‡çš„é‡è¦æ€§ï¼ˆå¦‚æœåŸºå‡†æƒé‡æ˜¾ç¤ºæ–œç‡å¾ˆé‡è¦ï¼‰
#     # æ–œç‡åŸºå‡†æƒé‡è¶Šå¤§ï¼Œè¯´æ˜è¶Šéœ€è¦é‡è§†ï¼Œè¿›ä¸€æ­¥æ”¾å¤§
#     slope_emphasis = base_w3 ** 0.5  # å¼€å¹³æ–¹æ ¹ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
#
#     adjusted_w = [
#         w[0] * 100 * base_w1,
#         w[1] * base_w2,
#         w[2] * base_w3  # é¢å¤–æ”¾å¤§æ–œç‡æƒé‡
#     ]
#
#     # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
#     total_w = sum(adjusted_w)
#     adjusted_w = [w / total_w for w in adjusted_w]
#
#     print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")
#
#     res = fit_inner_linear_model(
#         w=adjusted_w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_true_all,  # ä½¿ç”¨çœŸå®æ–œç‡è€Œä¸æ˜¯é¢„æµ‹æ–œç‡
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all
#     )
#
#     if not res.success:
#         print(f"è­¦å‘Š: ç¬¬ {i + 1} ç»„æƒé‡ä¼˜åŒ–å¤±è´¥: {res.message}")
#         continue
#
#     theta = res.x
#     beta, b = theta[:-1], theta[-1]
#
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}")
#
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = adjusted_w
#         best_theta = theta
#
# print(f"\nå¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.6f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
# res_final = fit_inner_linear_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_true_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
#
# theta_final = res_final.x
# beta_final, b_final = theta_final[:-1], theta_final[-1]
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
#
# # ===== 12. å¯¼å‡ºç»“æœ =====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all,
#     "Relative_Error_%": rel_err
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¼˜åŒ–å.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_ä¼˜åŒ–å.xlsx", index=False)
#
# # ===== 13. æœ€ç»ˆç»“æœæ±‡æ€» =====
# print("\n" + "=" * 60)
# print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€»")
# print("=" * 60)
# print(f"æœ€ä¼˜æƒé‡ç»„åˆ: w1={best_w[0]:.6f}, w2={best_w[1]:.6f}, w3={best_w[2]:.6f}")
# print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
# print(f"  RÂ²  = {r2_all:.6f}")
# print(f"  MSE = {mse_all:.6f}")
# print(f"  â‰¤1%: {(rel_err <= 1).sum()}/{len(rel_err)} ({(rel_err <= 1).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤5%: {(rel_err <= 5).sum()}/{len(rel_err)} ({(rel_err <= 5).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤10%: {(rel_err <= 10).sum()}/{len(rel_err)} ({(rel_err <= 10).sum() / len(rel_err) * 100:.2f}%)")
#
# # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
# mean_rel_err = np.mean(rel_err)
# median_rel_err = np.median(rel_err)
# print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
# print(f"  ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")
#
# # è®¡ç®—RÂ²_adjusted
# n_samples = len(y_exp)
# n_features = X_exp.shape[1]
# r2_adjusted = 1 - (1 - r2_all) * (n_samples - 1) / (n_samples - n_features - 1)
# print(f"  è°ƒæ•´åRÂ²: {r2_adjusted:.6f}")
#
# print("=" * 60)
# print("âœ… å®Œæˆï¼é¢„æµ‹ç»“æœå’Œç³»æ•°è¡¨å·²ä¿å­˜åˆ°Excelæ–‡ä»¶")


# very slow
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. Huber æŸå¤±å‡½æ•° =====
# def huber_loss(y_true, y_pred, delta=1.0):
#     """è®¡ç®— Huber æŸå¤±"""
#     error = np.abs(y_true - y_pred)
#     loss = np.where(error <= delta, 0.5 * error ** 2, delta * (error - 0.5 * delta))
#     return np.sum(loss)
#
#
# # ===== 7.1 ä¿®æ”¹ï¼šä¸‰é¡¹æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨ Huber æŸå¤± =====
# def loss_sum_three_parts_with_huber(y_exp_true, y_exp_pred,
#                                     Cp1_true, Cp2_true,
#                                     Cp1_pred, Cp2_pred,
#                                     slope_true, slope_pred,
#                                     w1, w2, w3, delta=1.0):
#     # ä½¿ç”¨ Huber æŸå¤±è®¡ç®—å„ä¸ªé¡¹çš„æŸå¤±
#     L_exp = huber_loss(y_exp_true, y_exp_pred, delta)
#     L_ref = huber_loss(Cp1_true, Cp1_pred, delta) + huber_loss(Cp2_true, Cp2_pred, delta)
#     L_slope = huber_loss(slope_true, slope_pred, delta)
#
#     # æ€»æŸå¤±æ˜¯åŠ æƒçš„ä¸‰é¡¹æŸå¤±
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # ===== 9. å†…å±‚ä¼˜åŒ–å‡½æ•°ï¼šç›®æ ‡å‡½æ•° =====
# def fit_inner_linear_model_with_huber(w, X_exp_train, y_exp_train, mat_idx_train,
#                                       Cp1_true_train, Cp2_true_train, slope_true_train,
#                                       T1_hat_all, T2_hat_all, delta=1.0):
#     w1, w2, w3 = w
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0
#
#     def objective(theta):
#         beta = theta[:-1]
#         b = theta[-1]
#
#         # å®éªŒç‚¹é¢„æµ‹
#         y_pred_train = X_exp_train @ beta + b
#
#         # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
#         unique_materials = np.unique(mat_idx_train)
#         Cp1_pred_train = np.zeros(len(unique_materials))
#         Cp2_pred_train = np.zeros(len(unique_materials))
#
#         for i, mat_idx in enumerate(unique_materials):
#             Nk = X_groups.iloc[mat_idx].values.astype(float)
#             s_feat = slope_feat_all[mat_idx]
#
#             x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#             x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#             Cp1_pred_train[i] = x_T1 @ beta + b
#             Cp2_pred_train[i] = x_T2 @ beta + b
#
#         # æ–œç‡é¢„æµ‹
#         slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
#                     T2_hat_all[unique_materials] - T1_hat_all[unique_materials])
#
#         # ä½¿ç”¨å¸¦ Huber æŸå¤±çš„ä¸‰é¡¹æŸå¤±
#         return loss_sum_three_parts_with_huber(
#             y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#             Cp1_true=Cp1_true_train[unique_materials],
#             Cp2_true=Cp2_true_train[unique_materials],
#             Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#             slope_true=slope_true_train[unique_materials],
#             slope_pred=slope_pred_train,
#             w1=w1, w2=w2, w3=w3, delta=delta
#         )
#
#     res = minimize(objective, theta0, method="Powell", options={"maxiter": 5000, "xtol": 1e-6, "ftol": 1e-6})
#     return res
#
#
# # ===== 9.2 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
# candidate_ws = sample_weight_triplets(n=100, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
# for i, w in enumerate(candidate_ws):
#     # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
#     total_w = sum(w)
#     adjusted_w = [wi / total_w for wi in w]
#
#     print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")
#
#     res = fit_inner_linear_model_with_huber(
#         w=adjusted_w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_true_all,  # ä½¿ç”¨çœŸå®æ–œç‡è€Œä¸æ˜¯é¢„æµ‹æ–œç‡
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all,
#         delta=1.0  # å¯ä»¥è°ƒæ•´deltaå€¼
#     )
#
#     if not res.success:
#         print(f"è­¦å‘Š: ç¬¬ {i + 1} ç»„æƒé‡ä¼˜åŒ–å¤±è´¥: {res.message}")
#         continue
#
#     theta = res.x
#     beta, b = theta[:-1], theta[-1]
#
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}")
#
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = adjusted_w
#         best_theta = theta
#
# print(f"\nå¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.6f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
# res_final = fit_inner_linear_model_with_huber(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_true_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all,
#     delta=1.0
# )
#
# theta_final = res_final.x
# beta_final, b_final = theta_final[:-1], theta_final[-1]
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
#
# # ===== 12. å¯¼å‡ºç»“æœ =====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all,
#     "Relative_Error_%": rel_err
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¼˜åŒ–å.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_ä¼˜åŒ–å.xlsx", index=False)
#
# # ===== 13. æœ€ç»ˆç»“æœæ±‡æ€» =====
# print("\n" + "=" * 60)
# print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€»")
# print("=" * 60)
# print(f"æœ€ä¼˜æƒé‡ç»„åˆ: w1={best_w[0]:.6f}, w2={best_w[1]:.6f}, w3={best_w[2]:.6f}")
# print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
# print(f"  RÂ²  = {r2_all:.6f}")
# print(f"  MSE = {mse_all:.6f}")
# print(f"  â‰¤1%: {(rel_err <= 1).sum()}/{len(rel_err)} ({(rel_err <= 1).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤5%: {(rel_err <= 5).sum()}/{len(rel_err)} ({(rel_err <= 5).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤10%: {(rel_err <= 10).sum()}/{len(rel_err)} ({(rel_err <= 10).sum() / len(rel_err) * 100:.2f}%)")
#
# # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
# mean_rel_err = np.mean(rel_err)
# median_rel_err = np.median(rel_err)
# print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
# print(f"  ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")
#
# # è®¡ç®—RÂ²_adjusted
# n_samples = len(y_exp)
# n_features = X_exp.shape[1]
# r2_adjusted = 1 - (1 - r2_all) * (n_samples - 1) / (n_samples - n_features - 1)
# print(f"  è°ƒæ•´åRÂ²: {r2_adjusted:.6f}")
#
# print("=" * 60)
# print("âœ… å®Œæˆï¼é¢„æµ‹ç»“æœå’Œç³»æ•°è¡¨å·²ä¿å­˜åˆ°Excelæ–‡ä»¶")

# huber with error
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. Huber æŸå¤±å‡½æ•° =====
# def huber_loss(y_true, y_pred, delta=1.0):
#     """è®¡ç®— Huber æŸå¤±"""
#     error = np.abs(y_true - y_pred)
#     loss = np.where(error <= delta, 0.5 * error ** 2, delta * (error - 0.5 * delta))
#     return np.sum(loss)
#
#
# # ===== 8. ä¿®æ­£ï¼šåŸºäºåæ¯”ä¾‹çš„è‡ªé€‚åº”æƒé‡è®¡ç®— =====
# def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
#     """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶ä½¿ç”¨åæ¯”ä¾‹è°ƒæ•´æƒé‡"""
#     L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡çº§
#     L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
#                      np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
#     L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§
#
#     L_exp_typical = max(L_exp_typical, 1e-10)
#     L_ref_typical = max(L_ref_typical, 1e-10)
#     L_slope_typical = max(L_slope_typical, 1e-10)
#
#     print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")
#
#     # ä½¿ç”¨åæ¯”ä¾‹å…³ç³»ï¼šæ•°å€¼è¶Šå°ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ”¾å¤§ä½œç”¨ï¼‰
#     w1_base = 1.0 / L_exp_typical  # å®éªŒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w2_base = 1.0 / L_ref_typical  # å‚è€ƒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w3_base = 1.0 / L_slope_typical  # æ–œç‡æ•°å€¼å°ï¼Œæƒé‡å¤§ï¼ˆæ”¾å¤§ï¼ï¼‰
#
#     # å½’ä¸€åŒ–
#     total_base = w1_base + w2_base + w3_base
#     w1_normalized = w1_base / total_base
#     w2_normalized = w2_base / total_base
#     w3_normalized = w3_base / total_base
#
#     return w1_normalized, w2_normalized, w3_normalized
#
#
# # ===== 8.1 è®¡ç®—æŸå¤±å€æ•°ï¼ˆç”¨äºæŸå¤±å‡½æ•°å†…éƒ¨æ”¾å¤§ï¼‰ =====
# def calculate_loss_multipliers(L_exp, L_ref, L_slope, max_multiplier=1000):
#     """è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°"""
#     max_loss = max(L_exp, L_ref, L_slope)
#
#     # è®¡ç®—éœ€è¦æ”¾å¤§çš„å€æ•°
#     multiplier_exp = max_loss / L_exp
#     multiplier_ref = max_loss / L_ref
#     multiplier_slope = max_loss / L_slope
#
#     # é™åˆ¶æœ€å¤§å€æ•°ï¼Œé¿å…æç«¯å€¼
#     multiplier_exp = min(multiplier_exp, max_multiplier)
#     multiplier_ref = min(multiplier_ref, max_multiplier)
#     multiplier_slope = min(multiplier_slope, max_multiplier)
#
#     return multiplier_exp, multiplier_ref, multiplier_slope
#
#
# # ===== 7.1 ä¿®æ­£ï¼šä½¿ç”¨æ”¾å¤§å€æ•°çš„æŸå¤±å‡½æ•° =====
# def loss_sum_three_parts_with_multipliers(y_exp_true, y_exp_pred,
#                                           Cp1_true, Cp2_true,
#                                           Cp1_pred, Cp2_pred,
#                                           slope_true, slope_pred,
#                                           w1, w2, w3):
#     """ä½¿ç”¨æ”¾å¤§å€æ•°åçš„æŸå¤±å‡½æ•°"""
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred)) * multiplier_exp
#     L_ref = (np.sum(np.abs(Cp1_true - Cp1_pred)) +
#              np.sum(np.abs(Cp2_true - Cp2_pred))) * multiplier_ref
#     L_slope = np.sum(np.abs(slope_true - slope_pred)) * multiplier_slope
#
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # è®¡ç®—åæ¯”ä¾‹åŸºå‡†æƒé‡
# base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
#     X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
# )
#
# # è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°
# L_exp, L_ref, L_slope = (
#     np.mean(np.abs(y_exp - np.mean(y_exp))),
#     (np.mean(np.abs(Cp1_true_all - np.mean(Cp1_true_all))) +
#      np.mean(np.abs(Cp2_true_all - np.mean(Cp2_true_all)))) / 2,
#     np.mean(np.abs(slope_true_all - np.mean(slope_true_all)))
# )
# multiplier_exp, multiplier_ref, multiplier_slope = calculate_loss_multipliers(L_exp, L_ref, L_slope)
#
# # ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
# candidate_ws = sample_weight_triplets(n=100, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
#
# # ===== 9.1 å†…å±‚ä¼˜åŒ–å‡½æ•°å®šä¹‰ =====
# def fit_inner_huber_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                           Cp1_true_train, Cp2_true_train, slope_true_train,
#                           T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#
#     # ä½¿ç”¨Huberå›å½’æ‹Ÿåˆä¸»æ¨¡å‹
#     huber_model = HuberRegressor(max_iter=1000, epsilon=1.35, alpha=0.0001)
#     huber_model.fit(X_exp_train, y_exp_train)
#
#     beta = huber_model.coef_
#     b = huber_model.intercept_
#
#     # è®¡ç®—å„é¡¹æŸå¤±
#     # å®éªŒç‚¹é¢„æµ‹
#     y_pred_train = X_exp_train @ beta + b
#
#     # å‚è€ƒç‚¹é¢„æµ‹
#     unique_materials = np.unique(mat_idx_train)
#     Cp1_pred_train = np.zeros(len(unique_materials))
#     Cp2_pred_train = np.zeros(len(unique_materials))
#
#     for i, mat_idx in enumerate(unique_materials):
#         Nk = X_groups.iloc[mat_idx].values.astype(float)
#         s_feat = slope_feat_all[mat_idx]
#
#         x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#         x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#         Cp1_pred_train[i] = x_T1 @ beta + b
#         Cp2_pred_train[i] = x_T2 @ beta + b
#
#     # æ–œç‡é¢„æµ‹
#     slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
#             T2_hat_all[unique_materials] - T1_hat_all[unique_materials])
#
#     # è®¡ç®—æ€»æŸå¤±
#     total_loss = loss_sum_three_parts_with_multipliers(
#         y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#         Cp1_true=Cp1_true_train[unique_materials],
#         Cp2_true=Cp2_true_train[unique_materials],
#         Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#         slope_true=slope_true_train[unique_materials],
#         slope_pred=slope_pred_train,
#         w1=w1, w2=w2, w3=w3
#     )
#
#     return beta, b, total_loss
#
#
# # ===== 9.2 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
# for i, w in enumerate(candidate_ws):
#     # ä½¿ç”¨å¹‚å‡½æ•°è¿›ä¸€æ­¥æ”¾å¤§æ–œç‡çš„é‡è¦æ€§ï¼ˆå¦‚æœåŸºå‡†æƒé‡æ˜¾ç¤ºæ–œç‡å¾ˆé‡è¦ï¼‰
#     # æ–œç‡åŸºå‡†æƒé‡è¶Šå¤§ï¼Œè¯´æ˜è¶Šéœ€è¦é‡è§†ï¼Œè¿›ä¸€æ­¥æ”¾å¤§
#     slope_emphasis = base_w3 ** 0.5  # å¼€å¹³æ–¹æ ¹ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
#
#     adjusted_w = [
#         w[0] * 100 * base_w1,
#         w[1] * base_w2,
#         w[2] * base_w3  # é¢å¤–æ”¾å¤§æ–œç‡æƒé‡
#     ]
#
#     # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
#     total_w = sum(adjusted_w)
#     adjusted_w = [w / total_w for w in adjusted_w]
#
#     print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")
#
#     beta, b, total_loss = fit_inner_huber_model(
#         w=adjusted_w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_true_all,
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all
#     )
#
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}, æ€»æŸå¤± = {total_loss:.6f}")
#
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = adjusted_w
#         best_beta = beta
#         best_b = b
#
# print(f"\nå¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.6f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
# beta_final, b_final, _ = fit_inner_huber_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_true_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
#
# # ===== 12. å¯¼å‡ºç»“æœ =====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all,
#     "Relative_Error_%": rel_err
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_Huberä¼˜åŒ–å.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_Huberä¼˜åŒ–å.xlsx", index=False)
#
# # ===== 13. æœ€ç»ˆç»“æœæ±‡æ€» =====
# print("\n" + "=" * 60)
# print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€» (Huberå›å½’)")
# print("=" * 60)
# print(f"æœ€ä¼˜æƒé‡ç»„åˆ: w1={best_w[0]:.6f}, w2={best_w[1]:.6f}, w3={best_w[2]:.6f}")
# print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
# print(f"  RÂ²  = {r2_all:.6f}")
# print(f"  MSE = {mse_all:.6f}")
# print(f"  â‰¤1%: {(rel_err <= 1).sum()}/{len(rel_err)} ({(rel_err <= 1).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤5%: {(rel_err <= 5).sum()}/{len(rel_err)} ({(rel_err <= 5).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤10%: {(rel_err <= 10).sum()}/{len(rel_err)} ({(rel_err <= 10).sum() / len(rel_err) * 100:.2f}%)")
#
# # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
# mean_rel_err = np.mean(rel_err)
# median_rel_err = np.median(rel_err)
# print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
# print(f"  ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")
#
# # è®¡ç®—RÂ²_adjusted
# n_samples = len(y_exp)
# n_features = X_exp.shape[1]
# r2_adjusted = 1 - (1 - r2_all) * (n_samples - 1) / (n_samples - n_features - 1)
# print(f"  è°ƒæ•´åRÂ²: {r2_adjusted:.6f}")
#
# print("=" * 60)
# print("âœ… å®Œæˆï¼é¢„æµ‹ç»“æœå’Œç³»æ•°è¡¨å·²ä¿å­˜åˆ°Excelæ–‡ä»¶")



#
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. ä¿®æ­£ï¼šåŸºäºåæ¯”ä¾‹çš„è‡ªé€‚åº”æƒé‡è®¡ç®— =====
# def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
#     """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶ä½¿ç”¨åæ¯”ä¾‹è°ƒæ•´æƒé‡"""
#     L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡çº§
#     L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
#                      np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
#     L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§
#
#     L_exp_typical = max(L_exp_typical, 1e-10)
#     L_ref_typical = max(L_ref_typical, 1e-10)
#     L_slope_typical = max(L_slope_typical, 1e-10)
#
#     print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")
#
#     # ä½¿ç”¨åæ¯”ä¾‹å…³ç³»ï¼šæ•°å€¼è¶Šå°ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ”¾å¤§ä½œç”¨ï¼‰
#     w1_base = 1.0 / L_exp_typical  # å®éªŒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w2_base = 1.0 / L_ref_typical  # å‚è€ƒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w3_base = 1.0 / L_slope_typical  # æ–œç‡æ•°å€¼å°ï¼Œæƒé‡å¤§ï¼ˆæ”¾å¤§ï¼ï¼‰
#
#     # å½’ä¸€åŒ–
#     total_base = w1_base + w2_base + w3_base
#     w1_normalized = w1_base / total_base
#     w2_normalized = w2_base / total_base
#     w3_normalized = w3_base / total_base
#
#     return w1_normalized, w2_normalized, w3_normalized
#
#
# # ===== 7.1 è®¡ç®—æŸå¤±å€æ•°ï¼ˆç”¨äºæŸå¤±å‡½æ•°å†…éƒ¨æ”¾å¤§ï¼‰ =====
# def calculate_loss_multipliers(L_exp, L_ref, L_slope, max_multiplier=1000):
#     """è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°"""
#     max_loss = max(L_exp, L_ref, L_slope)
#
#     # è®¡ç®—éœ€è¦æ”¾å¤§çš„å€æ•°
#     multiplier_exp = max_loss / L_exp
#     multiplier_ref = max_loss / L_ref
#     multiplier_slope = max_loss / L_slope
#
#     # é™åˆ¶æœ€å¤§å€æ•°ï¼Œé¿å…æç«¯å€¼
#     multiplier_exp = min(multiplier_exp, max_multiplier)
#     multiplier_ref = min(multiplier_ref, max_multiplier)
#     multiplier_slope = min(multiplier_slope, max_multiplier)
#
#     return multiplier_exp, multiplier_ref, multiplier_slope
#
#
# # è®¡ç®—åæ¯”ä¾‹åŸºå‡†æƒé‡
# base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
#     X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
# )
#
# # è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°
# L_exp, L_ref, L_slope = (
#     np.mean(np.abs(y_exp - np.mean(y_exp))),
#     (np.mean(np.abs(Cp1_true_all - np.mean(Cp1_true_all))) +
#      np.mean(np.abs(Cp2_true_all - np.mean(Cp2_true_all)))) / 2,
#     np.mean(np.abs(slope_true_all - np.mean(slope_true_all)))
# )
# multiplier_exp, multiplier_ref, multiplier_slope = calculate_loss_multipliers(L_exp, L_ref, L_slope)
#
#
# # ===== 8. è‡ªå®šä¹‰HuberæŸå¤±å‡½æ•°ï¼ˆæ•´åˆä¸‰é¡¹æŸå¤±ï¼‰ =====
# def custom_huber_loss_with_weights(theta, X, y, w1, w2, w3, mat_idx,
#                                    Cp1_true, Cp2_true, slope_true,
#                                    T1_hat, T2_hat, multiplier_exp,
#                                    multiplier_ref, multiplier_slope):
#     """
#     è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œå°†ä¸‰é¡¹æŸå¤±æ•´åˆåˆ°Huberå›å½’ä¸­
#     """
#     beta = theta[:-1]
#     b = theta[-1]
#
#     # å®éªŒç‚¹é¢„æµ‹å’ŒHuberæŸå¤±
#     y_pred = X @ beta + b
#     huber_delta = 1.0
#     error = np.abs(y - y_pred)
#     L_exp = np.where(error <= huber_delta, 0.5 * error ** 2, huber_delta * (error - 0.5 * huber_delta))
#     L_exp = np.sum(L_exp) * multiplier_exp
#
#     # å‚è€ƒç‚¹é¢„æµ‹å’ŒæŸå¤±
#     unique_materials = np.unique(mat_idx)
#     L_ref = 0
#     L_slope = 0
#
#     for mat_idx_val in unique_materials:
#         Nk = X_groups.iloc[mat_idx_val].values.astype(float)
#         s_feat = slope_feat_all[mat_idx_val]
#
#         # å‚è€ƒç‚¹T1
#         x_T1 = np.concatenate([Nk, Nk * T1_hat[mat_idx_val], [s_feat * T1_hat[mat_idx_val]]])
#         Cp1_pred = x_T1 @ beta + b
#         L_ref += np.abs(Cp1_true[mat_idx_val] - Cp1_pred)
#
#         # å‚è€ƒç‚¹T2
#         x_T2 = np.concatenate([Nk, Nk * T2_hat[mat_idx_val], [s_feat * T2_hat[mat_idx_val]]])
#         Cp2_pred = x_T2 @ beta + b
#         L_ref += np.abs(Cp2_true[mat_idx_val] - Cp2_pred)
#
#         # æ–œç‡æŸå¤±
#         if T2_hat[mat_idx_val] != T1_hat[mat_idx_val]:  # é¿å…é™¤é›¶
#             slope_pred = (Cp2_pred - Cp1_pred) / (T2_hat[mat_idx_val] - T1_hat[mat_idx_val])
#             L_slope += np.abs(slope_true[mat_idx_val] - slope_pred)
#
#     L_ref = L_ref * multiplier_ref
#     L_slope = L_slope * multiplier_slope
#
#     # åŠ æƒæ€»æŸå¤±
#     total_loss = w1 * L_exp + w2 * L_ref + w3 * L_slope
#     return total_loss
#
#
# # ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
# candidate_ws = sample_weight_triplets(n=10, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_beta = None
# best_b = None
#
#
# # ===== 9.1 å†…å±‚ä¼˜åŒ–å‡½æ•°å®šä¹‰ =====
# def fit_inner_custom_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                            Cp1_true_train, Cp2_true_train, slope_true_train,
#                            T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)
#
#     # ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–
#     res = minimize(
#         custom_huber_loss_with_weights,
#         theta0,
#         args=(X_exp_train, y_exp_train, w1, w2, w3, mat_idx_train,
#               Cp1_true_train, Cp2_true_train, slope_true_train,
#               T1_hat_all, T2_hat_all, multiplier_exp, multiplier_ref, multiplier_slope),
#         method="L-BFGS-B",
#         options={"maxiter": 10000, "ftol": 1e-3}
#     )
#
#     if res.success:
#         theta = res.x
#         beta = theta[:-1]
#         b = theta[-1]
#
#         # è®¡ç®—é¢„æµ‹å€¼å’ŒRÂ²
#         y_pred = X_exp_train @ beta + b
#         r2 = r2_score(y_exp_train, y_pred)
#
#         return beta, b, res.fun, r2
#     else:
#         raise ValueError(f"ä¼˜åŒ–å¤±è´¥: {res.message}")
#
#
# # ===== 9.2 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
# for i, w in enumerate(candidate_ws):
#     # è°ƒæ•´æƒé‡
#     adjusted_w = [
#         w[0] * 100*base_w1,
#         w[1] * base_w2,
#         w[2] * base_w3
#     ]
#
#     # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
#     total_w = sum(adjusted_w)
#     adjusted_w = [w / total_w for w in adjusted_w]
#
#     print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")
#
#     try:
#         beta, b, total_loss, r2 = fit_inner_custom_model(
#             w=adjusted_w,
#             X_exp_train=X_exp,
#             y_exp_train=y_exp,
#             mat_idx_train=mat_idx_per_sample,
#             Cp1_true_train=Cp1_true_all,
#             Cp2_true_train=Cp2_true_all,
#             slope_true_train=slope_true_all,
#             T1_hat_all=T1_hat_all,
#             T2_hat_all=T2_hat_all
#         )
#
#         print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}, æ€»æŸå¤± = {total_loss:.6f}")
#
#         if r2 > best_r2:
#             best_r2 = r2
#             best_w = adjusted_w
#             best_beta = beta
#             best_b = b
#
#     except Exception as e:
#         print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> ä¼˜åŒ–å¤±è´¥: {e}")
#         continue
#
# print(f"\nå¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.6f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
# beta_final, b_final, _, r2_final = fit_inner_custom_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_true_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
#
# # ===== 12. å¯¼å‡ºç»“æœ =====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all,
#     "Relative_Error_%": rel_err
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_Huberä¼˜åŒ–å.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_Huberä¼˜åŒ–å.xlsx", index=False)
#
# # ===== 13. æœ€ç»ˆç»“æœæ±‡æ€» =====
# print("\n" + "=" * 60)
# print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€» (è‡ªå®šä¹‰Huberå›å½’)")
# print("=" * 60)
# print(f"æœ€ä¼˜æƒé‡ç»„åˆ: w1={best_w[0]:.6f}, w2={best_w[1]:.6f}, w3={best_w[2]:.6f}")
# print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
# print(f"  RÂ²  = {r2_all:.6f}")
# print(f"  MSE = {mse_all:.6f}")
# print(f"  â‰¤1%: {(rel_err <= 1).sum()}/{len(rel_err)} ({(rel_err <= 1).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤5%: {(rel_err <= 5).sum()}/{len(rel_err)} ({(rel_err <= 5).sum() / len(rel_err) * 100:.2f}%)")
# print(f"  â‰¤10%: {(rel_err <= 10).sum()}/{len(rel_err)} ({(rel_err <= 10).sum() / len(rel_err) * 100:.2f}%)")
#
# # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
# mean_rel_err = np.mean(rel_err)
# median_rel_err = np.median(rel_err)
# print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
# print(f"  ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")
#
# # è®¡ç®—RÂ²_adjusted
# n_samples = len(y_exp)
# n_features = X_exp.shape[1]
# r2_adjusted = 1 - (1 - r2_all) * (n_samples - 1) / (n_samples - n_features - 1)
# print(f"  è°ƒæ•´åRÂ²: {r2_adjusted:.6f}")
#
# print("=" * 60)
# print("âœ… å®Œæˆï¼é¢„æµ‹ç»“æœå’Œç³»æ•°è¡¨å·²ä¿å­˜åˆ°Excelæ–‡ä»¶")
#
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. åŸºç¡€æŸå¤±å‡½æ•° =====
# def loss_sum_three_parts(y_exp_true, y_exp_pred,
#                          Cp1_true, Cp2_true,
#                          Cp1_pred, Cp2_pred,
#                          slope_true, slope_pred,
#                          w1, w2, w3):
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
#     L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
#     L_slope = np.sum(np.abs(slope_true - slope_pred))
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # ===== 8. ä¿®æ­£ï¼šåŸºäºåæ¯”ä¾‹çš„è‡ªé€‚åº”æƒé‡è®¡ç®— =====
# def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
#     """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶ä½¿ç”¨åæ¯”ä¾‹è°ƒæ•´æƒé‡"""
#     L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡çº§
#     L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
#                      np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
#     L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§
#
#     L_exp_typical = max(L_exp_typical, 1e-10)
#     L_ref_typical = max(L_ref_typical, 1e-10)
#     L_slope_typical = max(L_slope_typical, 1e-10)
#
#     print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")
#
#     # ä½¿ç”¨åæ¯”ä¾‹å…³ç³»ï¼šæ•°å€¼è¶Šå°ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ”¾å¤§ä½œç”¨ï¼‰
#     w1_base = 1.0/ L_exp_typical  # å®éªŒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w2_base = 1.0 / L_ref_typical  # å‚è€ƒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
#     w3_base = 1.0 / L_slope_typical  # æ–œç‡æ•°å€¼å°ï¼Œæƒé‡å¤§ï¼ˆæ”¾å¤§ï¼ï¼‰
#
#     # å½’ä¸€åŒ–
#     total_base = w1_base + w2_base + w3_base
#     w1_normalized = w1_base / total_base
#     w2_normalized = w2_base / total_base
#     w3_normalized = w3_base / total_base
#
#     return w1_normalized, w2_normalized, w3_normalized
#
#
# # è®¡ç®—åæ¯”ä¾‹åŸºå‡†æƒé‡
# base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
#     X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
# )
#
#
# # ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
# candidate_ws = sample_weight_triplets(n=10, seed=2025)  # éšæœºé‡‡æ ·æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
#
# # ===== 9.1 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
# for i, w in enumerate(candidate_ws):
#     adjusted_w = [w[0] * base_w1, w[1] * base_w2, w[2] * base_w3]  # å¯é€‰çš„æƒé‡è°ƒæ•´
#     adjusted_w = np.array(adjusted_w) / np.sum(adjusted_w)  # å½’ä¸€åŒ–
#
#     print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")
#
#     try:
#         # ä½¿ç”¨ Huber å›å½’æ¨¡å‹è¿›è¡Œä¼˜åŒ–
#         model_huber = HuberRegressor(epsilon=1.35, max_iter=10000, alpha=0.001)
#         model_huber.fit(X_exp, y_exp)
#
#         y_pred = model_huber.predict(X_exp)
#         r2 = r2_score(y_exp, y_pred)
#
#         print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}")
#
#         if r2 > best_r2:
#             best_r2 = r2
#             best_w = adjusted_w
#             best_theta = model_huber.coef_
#
#     except Exception as e:
#         print(f"ä¼˜åŒ–å¤±è´¥: {e}")
#         continue
#
# print(f"\næœ€ä¼˜æƒé‡ç»„åˆ: w* = {best_w}, æœ€å°ä¼˜åŒ–æŸå¤± = {best_r2:.6f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
#
# model_final = HuberRegressor(epsilon=1.35, max_iter=100, alpha=0.0001)
# model_final.fit(X_exp, y_exp)
# y_pred_final = model_final.predict(X_exp)
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# r2_final = r2_score(y_exp, y_pred_final)
#
# # è®¡ç®—ç›¸å¯¹è¯¯å·®
# rel_err = np.abs((y_pred_final - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
#
# # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
# mean_rel_err = np.mean(rel_err)
# median_rel_err = np.median(rel_err)
#
# print(f"æœ€ç»ˆæ¨¡å‹ RÂ² = {r2_final:.6f}")
# print(f"å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
# print(f"ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")

import numpy as np
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import HuberRegressor
from scipy.optimize import minimize


# ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
def sample_weight_triplets(n=2, seed=2025):
    rng = np.random.default_rng(seed)
    W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
    return W


# ===== 1. è¯»å–æ•°æ® =====
file_path = "heat capacity 207.xlsx"
df = pd.read_excel(file_path, sheet_name="Sheet1")
df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ===== 2. åˆ—å®šä¹‰ =====
group_cols = df.columns[11:30]
temp_cols = df.columns[30:40]
cp_cols = df.columns[40:50]
target_column_T1 = 'ASPEN Half Critical T'

material_id_col = df.columns[0]
material_ids_all = df[material_id_col].values

# ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
X_groups = df[group_cols]
valid_mask = ~df[target_column_T1].isna()

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_groups[valid_mask])

y_T1 = df.loc[valid_mask, target_column_T1].values
T1_model = GradientBoostingRegressor(
    n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0
).fit(X_poly, y_T1)

Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])

X_poly_all = poly.transform(X_groups)
T1_hat_all = T1_model.predict(X_poly_all)
T2_hat_all = 1.5 * T1_hat_all

Cp1_pred_all = Cp1_model.predict(X_groups)
Cp2_pred_all = Cp2_model.predict(X_groups)

# ===== 4. è®¡ç®—æ–œç‡ =====
with np.errstate(divide='ignore', invalid='ignore'):
    slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

# ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
Cp1_true_all = df.iloc[:, 9].astype(float).values
Cp2_true_all = df.iloc[:, 50].astype(float).values
T1_true_all = df[target_column_T1].astype(float).values
T2_true_all = 1.5 * T1_true_all

with np.errstate(divide='ignore', invalid='ignore'):
    slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)

# ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
for i in range(len(df)):
    Nk = X_groups.iloc[i].values.astype(float)
    s_feat = slope_feat_all[i]
    temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
    cps_i = df.loc[df.index[i], cp_cols].values.astype(float)

    for T, Cp in zip(temps_i, cps_i):
        if not (np.isfinite(T) and np.isfinite(Cp)):
            continue
        x = np.concatenate([Nk, Nk * T, [s_feat * T]])
        X_exp_list.append(x)
        y_exp_list.append(Cp)
        mat_idx_list.append(i)
        T_list.append(T)

X_exp = np.asarray(X_exp_list)
y_exp = np.asarray(y_exp_list)
mat_idx_per_sample = np.asarray(mat_idx_list)
T_per_sample = np.asarray(T_list)


# ===== 7. HuberæŸå¤±å‡½æ•° =====
def huber_loss(residuals, epsilon=1.35):
    abs_res = np.abs(residuals)
    return np.where(abs_res <= epsilon,
                    0.5 * residuals ** 2,
                    epsilon * (abs_res - 0.5 * epsilon))


# ===== 8. åŠ æƒæŸå¤±å‡½æ•° =====
def weighted_huber_loss(theta, X_exp, y_exp, mat_idx,
                        Cp1_true, Cp2_true, slope_true,
                        T1_hat, T2_hat, w1, w2, w3, alpha=0.0001, epsilon=1.35):
    """ä½¿ç”¨HuberæŸå¤±çš„åŠ æƒç›®æ ‡å‡½æ•°"""
    beta = theta[:-1]
    b = theta[-1]

    # å®éªŒç‚¹æŸå¤±
    y_pred_exp = X_exp @ beta + b
    exp_residuals = y_exp - y_pred_exp
    L_exp = np.sum(huber_loss(exp_residuals, epsilon))

    # å‚è€ƒç‚¹æŸå¤±
    unique_materials = np.unique(mat_idx)
    L_ref = 0
    L_slope = 0

    for mat_idx_val in unique_materials:
        Nk = X_groups.iloc[mat_idx_val].values.astype(float)
        s_feat = slope_feat_all[mat_idx_val]

        # å‚è€ƒç‚¹1
        x_T1 = np.concatenate([Nk, Nk * T1_hat[mat_idx_val], [s_feat * T1_hat[mat_idx_val]]])
        Cp1_pred = x_T1 @ beta + b
        ref1_residual = Cp1_true[mat_idx_val] - Cp1_pred
        L_ref += huber_loss(ref1_residual, epsilon)

        # å‚è€ƒç‚¹2
        x_T2 = np.concatenate([Nk, Nk * T2_hat[mat_idx_val], [s_feat * T2_hat[mat_idx_val]]])
        Cp2_pred = x_T2 @ beta + b
        ref2_residual = Cp2_true[mat_idx_val] - Cp2_pred
        L_ref += huber_loss(ref2_residual, epsilon)

        # æ–œç‡æŸå¤±
        if T2_hat[mat_idx_val] - T1_hat[mat_idx_val] > 1e-10:
            slope_pred = (Cp2_pred - Cp1_pred) / (T2_hat[mat_idx_val] - T1_hat[mat_idx_val])
            slope_residual = slope_true[mat_idx_val] - slope_pred
            L_slope += huber_loss(slope_residual, epsilon)

    # æ­£åˆ™åŒ–é¡¹
    regularization = alpha * np.sum(beta ** 2)

    return w1 * L_exp + w2 * L_ref + w3 * L_slope + regularization


# ===== 9. è‡ªé€‚åº”æƒé‡è®¡ç®— =====
def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
    L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))
    L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
                     np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2
    L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))

    L_exp_typical = max(L_exp_typical, 1e-10)
    L_ref_typical = max(L_ref_typical, 1e-10)
    L_slope_typical = max(L_slope_typical, 1e-10)

    print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")

    w1_base = 1.0 / L_exp_typical
    w2_base = 1.0 / L_ref_typical
    w3_base = 1.0 / L_slope_typical

    total_base = w1_base + w2_base + w3_base
    return w1_base / total_base, w2_base / total_base, w3_base / total_base


# è®¡ç®—åŸºå‡†æƒé‡
base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
    X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
)

# ===== 10. ä¼˜åŒ–å¾ªç¯ =====
candidate_ws = sample_weight_triplets(n=10, seed=2025)
best_w = None
best_r2 = -np.inf
best_theta = None

for i, w in enumerate(candidate_ws):
    adjusted_w = [w[0] *base_w1, w[1] * base_w2, w[2] * base_w3]
    adjusted_w = np.array(adjusted_w) / np.sum(adjusted_w)

    print(f"\nç¬¬{i + 1}ç»„æƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")

    # åˆå§‹å‚æ•°
    n_features = X_exp.shape[1]
    theta0 = np.zeros(n_features + 1)

    # ä½¿ç”¨L-BFGSä¼˜åŒ–ï¼ˆä¸Huberå›å½’ç›¸åŒçš„ç®—æ³•ï¼‰
    res = minimize(
        weighted_huber_loss,
        theta0,
        args=(X_exp, y_exp, mat_idx_per_sample,
              Cp1_true_all, Cp2_true_all, slope_true_all,
              T1_hat_all, T2_hat_all,
              adjusted_w[0], adjusted_w[1], adjusted_w[2],
              0.0001, 1.35),  # alpha=0.0001, epsilon=1.35
        method='L-BFGS-B',
        options={'maxiter': 10000, 'ftol': 1e-3, 'disp': False}
    )

    if not res.success:
        print(f"ä¼˜åŒ–å¤±è´¥: {res.message}")
        continue

    theta = res.x
    y_pred = X_exp @ theta[:-1] + theta[-1]
    r2 = r2_score(y_exp, y_pred)

    print(f"RÂ² = {r2:.6f}")

    if r2 > best_r2:
        best_r2 = r2
        best_w = adjusted_w
        best_theta = theta

print(f"\næœ€ä¼˜æƒé‡: {best_w}, RÂ² = {best_r2:.6f}")

# ===== 11. æœ€ç»ˆè¯„ä¼° =====
y_pred_final = X_exp @ best_theta[:-1] + best_theta[-1]
r2_final = r2_score(y_exp, y_pred_final)
rel_err = np.abs((y_pred_final - y_exp) / np.maximum(np.abs(y_exp), 1e-12)) * 100

print(f"\næœ€ç»ˆæ¨¡å‹ RÂ² = {r2_final:.6f}")
print(f"å¹³å‡ç›¸å¯¹è¯¯å·®: {np.mean(rel_err):.2f}%")
print(f"ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {np.median(rel_err):.2f}%")