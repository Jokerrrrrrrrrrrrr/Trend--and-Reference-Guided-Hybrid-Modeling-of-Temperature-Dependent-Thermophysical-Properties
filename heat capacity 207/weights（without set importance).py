# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.model_selection import GroupShuffleSplit
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1,w2,w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W  # shape: (n, 3)
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# # ä½¿ç”¨ GradientBoostingRegressor é¢„æµ‹ T1
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
# ).fit(X_poly, y_T1)
#
# # ä½¿ç”¨ HuberRegressor é¢„æµ‹ Cp1 å’Œ Cp2
# Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=9000).fit(X_groups, df.iloc[:, 50])
#
# # é¢„æµ‹ T1 å’Œ T2
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)  # é¢„æµ‹ T1
# T2_hat_all = 1.5 * T1_hat_all  # å‡è®¾ T2 = 1.5 * T1
#
# # é¢„æµ‹ Cp1 å’Œ Cp2
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# # æ¯ä¸ªç‰©è´¨çš„ T1 å’Œ T2 åº”è¯¥é‡å¤ä¸æ ·æœ¬æ•°é‡ä¸€è‡´çš„æ¬¡æ•°
# T1_hat_all_expanded = np.repeat(T1_hat_all, len(temp_cols))  # æ‰©å±• T1 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# T2_hat_all_expanded = np.repeat(T2_hat_all, len(temp_cols))  # æ‰©å±• T2 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# Cp1_pred_all_expanded = np.repeat(Cp1_pred_all, len(temp_cols))  # æ‰©å±• Cp1 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# Cp2_pred_all_expanded = np.repeat(Cp2_pred_all, len(temp_cols))  # æ‰©å±• Cp2 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
#
# # è®¡ç®—æ–œç‡ï¼šæ¯ä¸ªç‰©è´¨ä¸€ä¸ªæ–œç‡
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ (Cp1_true_all, Cp2_true_all) =====
# # çœŸå®çš„ Cp1 å’Œ Cp2
# Cp1_true_all = df.iloc[:, 9].astype(float).values  # å‚è€ƒç‚¹1çš„çœŸå® Cp
# Cp2_true_all = df.iloc[:, 50].astype(float).values  # å‚è€ƒç‚¹2çš„çœŸå® Cp
#
# # çœŸå®çš„ T1 å’Œ T2
# T1_true_all = df[target_column_T1].astype(float).values  # å‚è€ƒç‚¹1çš„çœŸå®æ¸©åº¦
# T2_true_all = 1.5 * T1_true_all  # å‡è®¾ T2 = 1.5 * T1
#
# # è®¡ç®—çœŸå®æ–œç‡
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»º"å®éªŒç‚¹æ ·æœ¬"ï¼šX(T) & yï¼ˆçº¿æ€§æ¨¡å‹çš„è¾“å…¥/è¾“å‡ºï¼‰ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]  # å›ºå®šç‰¹å¾ï¼Œä¸éšå†…å±‚å‚æ•°å˜åŒ–
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])  # 19 + 19 + 1 = 39 ç»´
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)  # è®°å½•è¯¥æ ·æœ¬å±äºå“ªä¸ªç‰©è´¨
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)  # (N_samples, 39)
# y_exp = np.asarray(y_exp_list)  # (N_samples,)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. æŸå¤±å‡½æ•°ï¼šè‡ªå®šä¹‰ä¸‰é¡¹æŸå¤±ï¼ˆä¸åšå¹³å‡ï¼‰ =====
# def loss_sum_three_parts(y_exp_true, y_exp_pred,
#                          Cp1_true, Cp2_true,
#                          Cp1_pred, Cp2_pred,
#                          slope_true, slope_pred,
#                          w1, w2, w3):
#     """
#     L = w1 * Î£|y_exp_true - y_exp_pred|
#       + w2 * Î£ ( |Cp1_true - Cp1_pred| + |Cp2_true - Cp2_pred| )
#       + w3 * Î£ |slope_true - slope_pred|  # æ–œç‡çš„åå·®ä½œä¸ºæŸå¤±é¡¹
#     """
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
#     L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
#     L_slope = np.sum(np.abs(slope_true - slope_pred))
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # ===== 8. å†…å±‚ä¼˜åŒ–ï¼šç»™å®š (w1, w2, w3)ï¼Œæœ€å°åŒ–æŸå¤±ï¼Œæ±‚çº¿æ€§æ¨¡å‹å‚æ•° =====
# def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                            Cp1_true_train, Cp2_true_train, slope_true_train,
#                            T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0
#
#     def objective(theta):
#         beta = theta[:-1]  # å›å½’ç³»æ•°
#         b = theta[-1]  # åç½®é¡¹
#
#         # å®éªŒç‚¹é¢„æµ‹
#         y_pred_train = X_exp_train @ beta + b
#
#         # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
#         # å¯¹äºæ¯ä¸ªç‰©è´¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å…¶åœ¨å‚è€ƒæ¸©åº¦ä¸‹çš„é¢„æµ‹å€¼
#         # é¦–å…ˆä¸ºæ¯ä¸ªç‰©è´¨æ„å»ºå‚è€ƒç‚¹ç‰¹å¾
#         Cp1_pred_train = np.zeros(len(np.unique(mat_idx_train)))
#         Cp2_pred_train = np.zeros(len(np.unique(mat_idx_train)))
#
#         for i, mat_idx in enumerate(np.unique(mat_idx_train)):
#             # è·å–è¯¥ç‰©è´¨çš„åŸºå›¢ç‰¹å¾
#             Nk = X_groups.iloc[mat_idx].values.astype(float)
#             s_feat = slope_feat_all[mat_idx]
#
#             # æ„å»º T1 å’Œ T2 çš„ç‰¹å¾å‘é‡
#             x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#             x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#             # é¢„æµ‹ Cp1 å’Œ Cp2
#             Cp1_pred_train[i] = x_T1 @ beta + b
#             Cp2_pred_train[i] = x_T2 @ beta + b
#
#         # æ–œç‡é¢„æµ‹
#         slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (T2_hat_all - T1_hat_all)
#
#         return loss_sum_three_parts(
#             y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#             Cp1_true=Cp1_true_train, Cp2_true=Cp2_true_train,
#             Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#             slope_true=slope_true_train, slope_pred=slope_pred_train,
#             w1=w1, w2=w2, w3=w3
#         )
#
#     res = minimize(objective, theta0, method="Powell", options={"maxiter": 100, "xtol": 1e-2, "ftol": 1e-2})
#     return res
#
#
# # ===== 9. å¤–å±‚ï¼šé€‰æ‹©æœ€ä¼˜ (w1, w2, w3) =====
# candidate_ws = sample_weight_triplets(n=40, seed=2025)
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
# for w in candidate_ws:
#     res = fit_inner_linear_model(
#         w=w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_feat_all,
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all
#     )
#     theta = res.x
#     beta, b = theta[:-1], theta[-1]
#
#     # ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆå¤–å±‚ç›®æ ‡ï¼‰
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     # è®°å½•æœ€ä¼˜
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = w
#         best_theta = theta
#
# print(f"å¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.4f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡ w* åœ¨"æ‰€æœ‰è®­ç»ƒæ ·æœ¬"ä¸Šé‡è®­ =====
# res_final = fit_inner_linear_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_feat_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
# theta_final = res_final.x
# beta_final, b_final = theta_final[:-1], theta_final[-1]
#
# # ===== 11. è®­ç»ƒé›†æ•´ä½“æ‹ŸåˆæŒ‡æ ‡ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
# print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹ï¼ˆç”¨ w* å†…å±‚é‡è®­åï¼‰")
# print(f"RÂ²  = {r2_all:.4f}")
# print(f"MSE = {mse_all:.4f}")
# print(f"â‰¤1%: {(rel_err <= 1).sum()}, â‰¤5%: {(rel_err <= 5).sum()}, â‰¤10%: {(rel_err <= 10).sum()}")
#
# # ===== 12. å¯¼å‡ºï¼ˆå¯é€‰ï¼‰=====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–_åˆ†ç»„ç•™å‡º.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–.xlsx", index=False)
#
# print("\nâœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–_åˆ†ç»„ç•™å‡º.xlsx")
# print("âœ… å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–.xlsx")


import numpy as np
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import HuberRegressor
from scipy.optimize import minimize


# ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
def sample_weight_triplets(n=2, seed=2025):  # å‡å°‘å¤–å±‚å¾ªç¯æ¬¡æ•°
    rng = np.random.default_rng(seed)
    W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
    return W


# ===== 1. è¯»å–æ•°æ® =====
file_path = "heat capacity 207.xlsx"
df = pd.read_excel(file_path, sheet_name="Sheet1")
df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ===== 2. åˆ—å®šä¹‰ =====
group_cols = df.columns[11:30]
temp_cols = df.columns[30:40]
cp_cols = df.columns[40:50]
target_column_T1 = 'ASPEN Half Critical T'

material_id_col = df.columns[0]
material_ids_all = df[material_id_col].values

# ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
X_groups = df[group_cols]
valid_mask = ~df[target_column_T1].isna()

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_groups[valid_mask])

y_T1 = df.loc[valid_mask, target_column_T1].values
T1_model = GradientBoostingRegressor(
    n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
).fit(X_poly, y_T1)

Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])

X_poly_all = poly.transform(X_groups)
T1_hat_all = T1_model.predict(X_poly_all)
T2_hat_all = 1.5 * T1_hat_all

Cp1_pred_all = Cp1_model.predict(X_groups)
Cp2_pred_all = Cp2_model.predict(X_groups)

# ===== 4. è®¡ç®—æ–œç‡ =====
with np.errstate(divide='ignore', invalid='ignore'):
    slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

# ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
Cp1_true_all = df.iloc[:, 9].astype(float).values
Cp2_true_all = df.iloc[:, 50].astype(float).values
T1_true_all = df[target_column_T1].astype(float).values
T2_true_all = 1.5 * T1_true_all

with np.errstate(divide='ignore', invalid='ignore'):
    slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)

# ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
for i in range(len(df)):
    Nk = X_groups.iloc[i].values.astype(float)
    s_feat = slope_feat_all[i]
    temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
    cps_i = df.loc[df.index[i], cp_cols].values.astype(float)

    for T, Cp in zip(temps_i, cps_i):
        if not (np.isfinite(T) and np.isfinite(Cp)):
            continue
        x = np.concatenate([Nk, Nk * T, [s_feat * T]])
        X_exp_list.append(x)
        y_exp_list.append(Cp)
        mat_idx_list.append(i)
        T_list.append(T)

X_exp = np.asarray(X_exp_list)
y_exp = np.asarray(y_exp_list)
mat_idx_per_sample = np.asarray(mat_idx_list)
T_per_sample = np.asarray(T_list)


# ===== 7. æŸå¤±å‡½æ•° =====
def loss_sum_three_parts(y_exp_true, y_exp_pred,
                         Cp1_true, Cp2_true,
                         Cp1_pred, Cp2_pred,
                         slope_true, slope_pred,
                         w1, w2, w3):
    L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
    L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
    L_slope = np.sum(np.abs(slope_true - slope_pred))
    return w1 * L_exp + w2 * L_ref + w3 * L_slope


# ===== 8. å†…å±‚ä¼˜åŒ–ï¼šä½¿ç”¨L-BFGS-Bç®—æ³• =====
def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
                           Cp1_true_train, Cp2_true_train, slope_true_train,
                           T1_hat_all, T2_hat_all):
    w1, w2, w3 = w
    n_feat = X_exp_train.shape[1]
    theta0 = np.zeros(n_feat + 1)

    # ä¸ºL-BFGS-Bå®šä¹‰è¾¹ç•Œï¼ˆå¯é€‰ï¼Œå¯ä»¥çº¦æŸå‚æ•°èŒƒå›´ï¼‰
    bounds = [(-10, 10)] * (n_feat + 1)  # æ‰€æœ‰å‚æ•°åœ¨-10åˆ°10ä¹‹é—´

    def objective(theta):
        beta = theta[:-1]
        b = theta[-1]

        # å®éªŒç‚¹é¢„æµ‹
        y_pred_train = X_exp_train @ beta + b

        # å‚è€ƒç‚¹é¢„æµ‹
        unique_materials = np.unique(mat_idx_train)
        Cp1_pred_train = np.zeros(len(unique_materials))
        Cp2_pred_train = np.zeros(len(unique_materials))

        for i, mat_idx in enumerate(unique_materials):
            Nk = X_groups.iloc[mat_idx].values.astype(float)
            s_feat = slope_feat_all[mat_idx]

            x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
            x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])

            Cp1_pred_train[i] = x_T1 @ beta + b
            Cp2_pred_train[i] = x_T2 @ beta + b

        # æ–œç‡é¢„æµ‹
        slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
                    T2_hat_all[unique_materials] - T1_hat_all[unique_materials])

        return loss_sum_three_parts(
            y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
            Cp1_true=Cp1_true_train[unique_materials],
            Cp2_true=Cp2_true_train[unique_materials],
            Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
            slope_true=slope_true_train[unique_materials],
            slope_pred=slope_pred_train,
            w1=w1, w2=w2, w3=w3
        )

    # ä½¿ç”¨L-BFGS-Bç®—æ³•ï¼ˆæ›´å¿«ï¼ï¼‰
    res = minimize(objective, theta0, method="L-BFGS-B", bounds=bounds,
                   options={"maxiter": 200, "ftol": 1e-4, "gtol": 1e-4, "disp": True})

    return res


# ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆå‡å°‘å¾ªç¯æ¬¡æ•°ï¼‰ =====
candidate_ws = sample_weight_triplets(n=10, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡

best_w = None
best_r2 = -np.inf
best_theta = None

for i, w in enumerate(candidate_ws):
    print(f"æ­£åœ¨æµ‹è¯•ç¬¬ {i + 1}/10 ç»„æƒé‡: {w}")

    res = fit_inner_linear_model(
        w=w,
        X_exp_train=X_exp,
        y_exp_train=y_exp,
        mat_idx_train=mat_idx_per_sample,
        Cp1_true_train=Cp1_true_all,
        Cp2_true_train=Cp2_true_all,
        slope_true_train=slope_feat_all,
        T1_hat_all=T1_hat_all,
        T2_hat_all=T2_hat_all
    )

    if not res.success:
        print(f"è­¦å‘Š: ç¬¬ {i + 1} ç»„æƒé‡ä¼˜åŒ–å¤±è´¥: {res.message}")
        continue

    theta = res.x
    beta, b = theta[:-1], theta[-1]

    y_val_pred = X_exp @ beta + b
    r2 = r2_score(y_exp, y_val_pred)

    print(f"æƒé‡ {w} -> RÂ² = {r2:.4f}")

    if r2 > best_r2:
        best_r2 = r2
        best_w = w
        best_theta = theta

print(f"å¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.4f}")

# ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
print("ä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
res_final = fit_inner_linear_model(
    w=best_w,
    X_exp_train=X_exp,
    y_exp_train=y_exp,
    mat_idx_train=mat_idx_per_sample,
    Cp1_true_train=Cp1_true_all,
    Cp2_true_train=Cp2_true_all,
    slope_true_train=slope_feat_all,
    T1_hat_all=T1_hat_all,
    T2_hat_all=T2_hat_all
)

theta_final = res_final.x
beta_final, b_final = theta_final[:-1], theta_final[-1]

# ===== 11. è¯„ä¼°ç»“æœ =====
y_pred_all = X_exp @ beta_final + b_final
mse_all = mean_squared_error(y_exp, y_pred_all)
r2_all = r2_score(y_exp, y_pred_all)

rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹ç»“æœ")
print(f"RÂ²  = {r2_all:.4f}")
print(f"MSE = {mse_all:.4f}")
print(f"â‰¤1%: {(rel_err <= 1).sum()}, â‰¤5%: {(rel_err <= 5).sum()}, â‰¤10%: {(rel_err <= 10).sum()}")

# ===== 12. å¯¼å‡ºç»“æœ =====
results = pd.DataFrame({
    "Material_ID": material_ids_all[mat_idx_per_sample],
    "Temperature (K)": T_per_sample,
    "Cp_measured": y_exp,
    "Cp_predicted": y_pred_all
})
results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¼˜åŒ–å.xlsx", index=False)

feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
coef_df.to_excel("Cpç³»æ•°è¡¨_ä¼˜åŒ–å.xlsx", index=False)

print("\nâœ… å®Œæˆï¼")