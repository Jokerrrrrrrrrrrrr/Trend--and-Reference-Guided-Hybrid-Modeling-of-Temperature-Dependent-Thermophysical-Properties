# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1,w2,w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W  # shape: (n, 3)
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# # ä½¿ç”¨ GradientBoostingRegressor é¢„æµ‹ T1
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
# ).fit(X_poly, y_T1)
#
# # ä½¿ç”¨ HuberRegressor é¢„æµ‹ Cp1 å’Œ Cp2
# Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=9000).fit(X_groups, df.iloc[:, 50])
#
# # é¢„æµ‹ T1 å’Œ T2
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)  # é¢„æµ‹ T1
# T2_hat_all = 1.5 * T1_hat_all  # å‡è®¾ T2 = 1.5 * T1
#
# # é¢„æµ‹ Cp1 å’Œ Cp2
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# # æ¯ä¸ªç‰©è´¨çš„ T1 å’Œ T2 åº”è¯¥é‡å¤ä¸æ ·æœ¬æ•°é‡ä¸€è‡´çš„æ¬¡æ•°
# T1_hat_all_expanded = np.repeat(T1_hat_all, len(temp_cols))  # æ‰©å±• T1 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# T2_hat_all_expanded = np.repeat(T2_hat_all, len(temp_cols))  # æ‰©å±• T2 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# Cp1_pred_all_expanded = np.repeat(Cp1_pred_all, len(temp_cols))  # æ‰©å±• Cp1 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
# Cp2_pred_all_expanded = np.repeat(Cp2_pred_all, len(temp_cols))  # æ‰©å±• Cp2 ä¸ºä¸æ ·æœ¬æ•°åŒ¹é…
#
# # è®¡ç®—æ–œç‡ï¼šæ¯ä¸ªç‰©è´¨ä¸€ä¸ªæ–œç‡
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ (Cp1_true_all, Cp2_true_all) =====
# # çœŸå®çš„ Cp1 å’Œ Cp2
# Cp1_true_all = df.iloc[:, 9].astype(float).values  # å‚è€ƒç‚¹1çš„çœŸå® Cp
# Cp2_true_all = df.iloc[:, 50].astype(float).values  # å‚è€ƒç‚¹2çš„çœŸå® Cp
#
# # çœŸå®çš„ T1 å’Œ T2
# T1_true_all = df[target_column_T1].astype(float).values  # å‚è€ƒç‚¹1çš„çœŸå®æ¸©åº¦
# T2_true_all = 1.5 * T1_true_all  # å‡è®¾ T2 = 1.5 * T1
#
# # è®¡ç®—çœŸå®æ–œç‡
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»º"å®éªŒç‚¹æ ·æœ¬"ï¼šX(T) & yï¼ˆçº¿æ€§æ¨¡å‹çš„è¾“å…¥/è¾“å‡ºï¼‰ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]  # å›ºå®šç‰¹å¾ï¼Œä¸éšå†…å±‚å‚æ•°å˜åŒ–
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])  # 19 + 19 + 1 = 39 ç»´
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)  # è®°å½•è¯¥æ ·æœ¬å±äºå“ªä¸ªç‰©è´¨
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)  # (N_samples, 39)
# y_exp = np.asarray(y_exp_list)  # (N_samples,)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. æŸå¤±å‡½æ•°ï¼šè‡ªå®šä¹‰ä¸‰é¡¹æŸå¤±ï¼ˆä¸åšå¹³å‡ï¼‰ =====
# def loss_sum_three_parts(y_exp_true, y_exp_pred,
#                          Cp1_true, Cp2_true,
#                          Cp1_pred, Cp2_pred,
#                          slope_true, slope_pred,
#                          w1, w2, w3):
#     """
#     L = w1 * Î£|y_exp_true - y_exp_pred|
#       + w2 * Î£ ( |Cp1_true - Cp1_pred| + |Cp2_true - Cp2_pred| )
#       + w3 * Î£ |slope_true - slope_pred|  # æ–œç‡çš„åå·®ä½œä¸ºæŸå¤±é¡¹
#     """
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
#     L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
#     L_slope = np.sum(np.abs(slope_true - slope_pred))
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # ===== 8. å†…å±‚ä¼˜åŒ–ï¼šç»™å®š (w1, w2, w3)ï¼Œæœ€å°åŒ–æŸå¤±ï¼Œæ±‚çº¿æ€§æ¨¡å‹å‚æ•° =====
# def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                            Cp1_true_train, Cp2_true_train, slope_true_train,
#                            T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0
#
#     def objective(theta):
#         beta = theta[:-1]  # å›å½’ç³»æ•°
#         b = theta[-1]  # åç½®é¡¹
#
#         # å®éªŒç‚¹é¢„æµ‹
#         y_pred_train = X_exp_train @ beta + b
#
#         # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
#         Cp1_pred_train = np.zeros(len(np.unique(mat_idx_train)))
#         Cp2_pred_train = np.zeros(len(np.unique(mat_idx_train)))
#
#         for i, mat_idx in enumerate(np.unique(mat_idx_train)):
#             # è·å–è¯¥ç‰©è´¨çš„åŸºå›¢ç‰¹å¾
#             Nk = X_groups.iloc[mat_idx].values.astype(float)
#             s_feat = slope_feat_all[mat_idx]
#
#             # æ„å»º T1 å’Œ T2 çš„ç‰¹å¾å‘é‡
#             x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#             x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#             # é¢„æµ‹ Cp1 å’Œ Cp2
#             Cp1_pred_train[i] = x_T1 @ beta + b
#             Cp2_pred_train[i] = x_T2 @ beta + b
#
#         # æ–œç‡é¢„æµ‹
#         slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (T2_hat_all - T1_hat_all)
#
#         return loss_sum_three_parts(
#             y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#             Cp1_true=Cp1_true_train, Cp2_true=Cp2_true_train,
#             Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#             slope_true=slope_true_train, slope_pred=slope_pred_train,
#             w1=w1, w2=w2, w3=w3
#         )
#
#     # ä½¿ç”¨ L-BFGS-B ä¼˜åŒ–ç®—æ³•
#     res = minimize(objective, theta0, method="L-BFGS-B", options={"maxiter": 5000, "ftol": 1e-6, "gtol": 1e-6})
#     return res
#
#
# # ===== 9. å¤–å±‚ï¼šé€‰æ‹©æœ€ä¼˜ (w1, w2, w3) =====
# candidate_ws = sample_weight_triplets(n=40, seed=2025)
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
# for w in candidate_ws:
#     res = fit_inner_linear_model(
#         w=w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_feat_all,
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all
#     )
#     theta = res.x
#     beta, b = theta[:-1], theta[-1]
#
#     # ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆå¤–å±‚ç›®æ ‡ï¼‰
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     # è®°å½•æœ€ä¼˜
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = w
#         best_theta = theta
#
# print(f"å¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.4f}")
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡ w* åœ¨"æ‰€æœ‰è®­ç»ƒæ ·æœ¬"ä¸Šé‡è®­ =====
# res_final = fit_inner_linear_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_feat_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
# theta_final = res_final.x
# beta_final, b_final = theta_final[:-1], theta_final[-1]
#
# # ===== 11. è®­ç»ƒé›†æ•´ä½“æ‹ŸåˆæŒ‡æ ‡ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
# print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹ï¼ˆç”¨ w* å†…å±‚é‡è®­åï¼‰")
# print(f"RÂ²  = {r2_all:.4f}")
# print(f"MSE = {mse_all:.4f}")
# print(f"â‰¤1%: {(rel_err <= 1).sum()}, â‰¤5%: {(rel_err <= 5).sum()}, â‰¤10%: {(rel_err <= 10).sum()}")
#
# # ===== 12. å¯¼å‡ºï¼ˆå¯é€‰ï¼‰=====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–_åˆ†ç»„ç•™å‡º.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–.xlsx", index=False)
#
# print("\nâœ… å·²ä¿å­˜ï¼šCpé¢„æµ‹ç»“æœ_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–_åˆ†ç»„ç•™å‡º.xlsx")
# print("âœ… å·²ä¿å­˜ï¼šCpç³»æ•°è¡¨_ä¸‰é¡¹æŸå¤±_åŒå±‚ä¼˜åŒ–.xlsx")




#not suitble because of çº¿æ€§å åŠ 
# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.metrics import r2_score, mean_squared_error
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import HuberRegressor
# from scipy.optimize import minimize
#
#
# # ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
# def sample_weight_triplets(n=2, seed=2025):
#     """
#     é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
#     ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
#     """
#     rng = np.random.default_rng(seed)
#     W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
#     return W
#
#
# # ===== 1. è¯»å–æ•°æ® =====
# file_path = "heat capacity 207.xlsx"
# df = pd.read_excel(file_path, sheet_name="Sheet1")
# df = df.dropna(subset=[df.columns[0]])
# df[df.columns[0]] = df[df.columns[0]].astype(int)
#
# # ===== 2. åˆ—å®šä¹‰ =====
# group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
# temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
# cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
# target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰
#
# material_id_col = df.columns[0]
# material_ids_all = df[material_id_col].values
#
#
# # ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
# X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
# valid_mask = ~df[target_column_T1].isna()
#
# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(X_groups[valid_mask])
#
# y_T1 = df.loc[valid_mask, target_column_T1].values
# T1_model = GradientBoostingRegressor(
#     n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
# ).fit(X_poly, y_T1)
#
# Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
# Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])
#
# X_poly_all = poly.transform(X_groups)
# T1_hat_all = T1_model.predict(X_poly_all)
# T2_hat_all = 1.5 * T1_hat_all
#
# Cp1_pred_all = Cp1_model.predict(X_groups)
# Cp2_pred_all = Cp2_model.predict(X_groups)
#
# # ===== 4. è®¡ç®—æ–œç‡ =====
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# # ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
# Cp1_true_all = df.iloc[:, 9].astype(float).values
# Cp2_true_all = df.iloc[:, 50].astype(float).values
# T1_true_all = df[target_column_T1].astype(float).values
# T2_true_all = 1.5 * T1_true_all
#
# with np.errstate(divide='ignore', invalid='ignore'):
#     slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)
#
# # ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
# slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)
#
# X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
# for i in range(len(df)):
#     Nk = X_groups.iloc[i].values.astype(float)
#     s_feat = slope_feat_all[i]
#     temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
#     cps_i = df.loc[df.index[i], cp_cols].values.astype(float)
#
#     for T, Cp in zip(temps_i, cps_i):
#         if not (np.isfinite(T) and np.isfinite(Cp)):
#             continue
#         x = np.concatenate([Nk, Nk * T, [s_feat * T]])
#         X_exp_list.append(x)
#         y_exp_list.append(Cp)
#         mat_idx_list.append(i)
#         T_list.append(T)
#
# X_exp = np.asarray(X_exp_list)
# y_exp = np.asarray(y_exp_list)
# mat_idx_per_sample = np.asarray(mat_idx_list)
# T_per_sample = np.asarray(T_list)
#
#
# # ===== 7. æŸå¤±å‡½æ•° =====
# def loss_sum_three_parts(y_exp_true, y_exp_pred,
#                          Cp1_true, Cp2_true,
#                          Cp1_pred, Cp2_pred,
#                          slope_true, slope_pred,
#                          w1, w2, w3):
#     L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
#     L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
#     L_slope = np.sum(np.abs(slope_true - slope_pred))
#     return w1 * L_exp + w2 * L_ref + w3 * L_slope
#
#
# # ===== 8. è‡ªé€‚åº”æƒé‡è®¡ç®—ï¼šåŸºäºæŸå¤±é¡¹çš„æ•°é‡çº§ =====
# def calculate_adaptive_weights(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
#     """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶è‡ªé€‚åº”è°ƒæ•´æƒé‡"""
#     L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡çº§
#     L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) + np.mean(
#         np.abs(Cp2_true - np.mean(Cp2_true))))/2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
#     L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§
#
#     L_exp_typical = max(L_exp_typical, 1e-10)
#     L_ref_typical = max(L_ref_typical, 1e-10)
#     L_slope_typical = max(L_slope_typical, 1e-10)
#
#     # è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æƒé‡æ¯”ä¾‹ï¼ˆä½¿å¾—å„ä¸ªæŸå¤±é¡¹çš„è´¡çŒ®ç›¸å½“ï¼‰
#     total = L_exp_typical + L_ref_typical + L_slope_typical
#     w1_base = L_exp_typical / total
#     w2_base = L_ref_typical / total
#     w3_base = L_slope_typical / total
#
#     return w1_base, w2_base, w3_base
#
#
# # è®¡ç®—è‡ªé€‚åº”åŸºå‡†æƒé‡
# base_w1, base_w2, base_w3 = calculate_adaptive_weights(
#     X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
# )
# print(f"è‡ªé€‚åº”åŸºå‡†æƒé‡: w1={base_w1:.4f}, w2={base_w2:.4f}, w3={base_w3:.4f}")
#
#
# # ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
# candidate_ws = sample_weight_triplets(n=10, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡
#
# best_w = None
# best_r2 = -np.inf
# best_theta = None
#
# # fit_inner_linear_model å¿…é¡»å®šä¹‰
# def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
#                            Cp1_true_train, Cp2_true_train, slope_true_train,
#                            T1_hat_all, T2_hat_all):
#     w1, w2, w3 = w
#     n_feat = X_exp_train.shape[1]
#     theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0
#
#     def objective(theta):
#         beta = theta[:-1]
#         b = theta[-1]
#
#         # å®éªŒç‚¹é¢„æµ‹
#         y_pred_train = X_exp_train @ beta + b
#
#         # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
#         unique_materials = np.unique(mat_idx_train)
#         Cp1_pred_train = np.zeros(len(unique_materials))
#         Cp2_pred_train = np.zeros(len(unique_materials))
#
#         for i, mat_idx in enumerate(unique_materials):
#             Nk = X_groups.iloc[mat_idx].values.astype(float)
#             s_feat = slope_feat_all[mat_idx]
#
#             x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
#             x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])
#
#             Cp1_pred_train[i] = x_T1 @ beta + b
#             Cp2_pred_train[i] = x_T2 @ beta + b
#
#         # æ–œç‡é¢„æµ‹
#         slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
#                     T2_hat_all[unique_materials] - T1_hat_all[unique_materials])
#
#         return loss_sum_three_parts(
#             y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
#             Cp1_true=Cp1_true_train[unique_materials],
#             Cp2_true=Cp2_true_train[unique_materials],
#             Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
#             slope_true=slope_true_train[unique_materials],
#             slope_pred=slope_pred_train,
#             w1=w1, w2=w2, w3=w3
#         )
#
#     res = minimize(objective, theta0, method="Powell", options={"maxiter": 5000, "xtol": 1e-6, "ftol": 1e-6})
#     return res
#
#
# for i, w in enumerate(candidate_ws):
#     # å°†è‡ªé€‚åº”åŸºå‡†æƒé‡å’Œé‡‡æ ·çš„æƒé‡ç»“åˆ
#     adjusted_w = [
#         w[0] * base_w1,
#         w[1] * base_w2,
#         w[2] * base_w3,
#     ]
#
#     # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
#     total_w = sum(adjusted_w)
#     adjusted_w = [w / total_w for w in adjusted_w]
#
#     res = fit_inner_linear_model(
#         w=adjusted_w,
#         X_exp_train=X_exp,
#         y_exp_train=y_exp,
#         mat_idx_train=mat_idx_per_sample,
#         Cp1_true_train=Cp1_true_all,
#         Cp2_true_train=Cp2_true_all,
#         slope_true_train=slope_feat_all,
#         T1_hat_all=T1_hat_all,
#         T2_hat_all=T2_hat_all
#     )
#
#     if not res.success:
#         print(f"è­¦å‘Š: ç¬¬ {i + 1} ç»„æƒé‡ä¼˜åŒ–å¤±è´¥: {res.message}")
#         continue
#
#     theta = res.x
#     beta, b = theta[:-1], theta[-1]
#
#     y_val_pred = X_exp @ beta + b
#     r2 = r2_score(y_exp, y_val_pred)
#
#     print(f"æƒé‡ {adjusted_w} -> RÂ² = {r2:.4f}")
#
#     if r2 > best_r2:
#         best_r2 = r2
#         best_w = adjusted_w
#         best_theta = theta
#
# print(f"å¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.4f}")
#
#
# # ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
# res_final = fit_inner_linear_model(
#     w=best_w,
#     X_exp_train=X_exp,
#     y_exp_train=y_exp,
#     mat_idx_train=mat_idx_per_sample,
#     Cp1_true_train=Cp1_true_all,
#     Cp2_true_train=Cp2_true_all,
#     slope_true_train=slope_feat_all,
#     T1_hat_all=T1_hat_all,
#     T2_hat_all=T2_hat_all
# )
#
# theta_final = res_final.x
# beta_final, b_final = theta_final[:-1], theta_final[-1]
#
# # ===== 11. è¯„ä¼°ç»“æœ =====
# y_pred_all = X_exp @ beta_final + b_final
# mse_all = mean_squared_error(y_exp, y_pred_all)
# r2_all = r2_score(y_exp, y_pred_all)
#
# rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100
# print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹ç»“æœ")
# print(f"RÂ²  = {r2_all:.4f}")
# print(f"MSE = {mse_all:.4f}")
# print(f"â‰¤1%: {(rel_err <= 1).sum()}, â‰¤5%: {(rel_err <= 5).sum()}, â‰¤10%: {(rel_err <= 10).sum()}")
#
# # ===== 12. å¯¼å‡ºç»“æœ =====
# results = pd.DataFrame({
#     "Material_ID": material_ids_all[mat_idx_per_sample],
#     "Temperature (K)": T_per_sample,
#     "Cp_measured": y_exp,
#     "Cp_predicted": y_pred_all
# })
# results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¼˜åŒ–å.xlsx", index=False)
#
# feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
# coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
# coef_df.to_excel("Cpç³»æ•°è¡¨_ä¼˜åŒ–å.xlsx", index=False)
#
# print("\nâœ… å®Œæˆï¼")

import numpy as np
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import HuberRegressor, LinearRegression
from scipy.optimize import minimize


# ===== 0. å·¥å…·ï¼šå€™é€‰æƒé‡é‡‡æ ·ï¼ˆå¤–å±‚ï¼‰ =====
def sample_weight_triplets(n=2, seed=2025):
    """
    é‡‡æ · n ç»„ (w1, w2, w3)ï¼Œéè´Ÿä¸”å’Œä¸º1ï¼ˆé¿å…å…¨0é€€åŒ–ï¼‰ã€‚
    ç”¨ Dirichlet(1,1,1) éšæœºæƒé‡ï¼Œè¦†ç›–é¢æ›´å‡åŒ€ã€‚
    """
    rng = np.random.default_rng(seed)
    W = rng.dirichlet([1.0, 1.0, 1.0], size=n)
    return W


# ===== 1. è¯»å–æ•°æ® =====
file_path = "heat capacity 207.xlsx"
df = pd.read_excel(file_path, sheet_name="Sheet1")
df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ===== 2. åˆ—å®šä¹‰ =====
group_cols = df.columns[11:30]  # 19 ä¸ªåŸºå›¢ç‰¹å¾
temp_cols = df.columns[30:40]  # 10 ä¸ªå®éªŒæ¸©åº¦ç‚¹ï¼ˆç½‘æ ¼ï¼‰
cp_cols = df.columns[40:50]  # 10 ä¸ªå®éªŒ Cpï¼ˆå¯¹åº”ä¸Šé¢æ¸©åº¦ç½‘æ ¼ï¼‰
target_column_T1 = 'ASPEN Half Critical T'  # ä½œä¸º T1_true çš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è¯¥ç‰©è´¨åœ¨å‚è€ƒé¡¹/æ–œç‡é¡¹é‡Œè·³è¿‡ï¼‰

material_id_col = df.columns[0]
material_ids_all = df[material_id_col].values

# ===== 3. å­æ¨¡å‹è®­ç»ƒ =====
X_groups = df[group_cols]  # åŸºå›¢ç‰¹å¾
valid_mask = ~df[target_column_T1].isna()

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_groups[valid_mask])

y_T1 = df.loc[valid_mask, target_column_T1].values
T1_model = GradientBoostingRegressor(
    n_estimators=100, learning_rate=0.1, max_depth=4, random_state=0  # ç®€åŒ–æ¨¡å‹
).fit(X_poly, y_T1)

Cp1_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 9])
Cp2_model = HuberRegressor(max_iter=1000).fit(X_groups, df.iloc[:, 50])

X_poly_all = poly.transform(X_groups)
T1_hat_all = T1_model.predict(X_poly_all)
T2_hat_all = 1.5 * T1_hat_all

Cp1_pred_all = Cp1_model.predict(X_groups)
Cp2_pred_all = Cp2_model.predict(X_groups)

# ===== 4. è®¡ç®—æ–œç‡ =====
with np.errstate(divide='ignore', invalid='ignore'):
    slope_pred_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

# ===== 5. çœŸå®å‚è€ƒç‚¹å€¼ =====
Cp1_true_all = df.iloc[:, 9].astype(float).values
Cp2_true_all = df.iloc[:, 50].astype(float).values
T1_true_all = df[target_column_T1].astype(float).values
T2_true_all = 1.5 * T1_true_all

with np.errstate(divide='ignore', invalid='ignore'):
    slope_true_all = (Cp2_true_all - Cp1_true_all) / (T2_true_all - T1_true_all)

# ===== 6. æ„å»ºå®éªŒç‚¹æ ·æœ¬ =====
slope_feat_all = (Cp2_pred_all - Cp1_pred_all) / (T2_hat_all - T1_hat_all)

X_exp_list, y_exp_list, mat_idx_list, T_list = [], [], [], []
for i in range(len(df)):
    Nk = X_groups.iloc[i].values.astype(float)
    s_feat = slope_feat_all[i]
    temps_i = df.loc[df.index[i], temp_cols].values.astype(float)
    cps_i = df.loc[df.index[i], cp_cols].values.astype(float)

    for T, Cp in zip(temps_i, cps_i):
        if not (np.isfinite(T) and np.isfinite(Cp)):
            continue
        x = np.concatenate([Nk, Nk * T, [s_feat * T]])
        X_exp_list.append(x)
        y_exp_list.append(Cp)
        mat_idx_list.append(i)
        T_list.append(T)

X_exp = np.asarray(X_exp_list)
y_exp = np.asarray(y_exp_list)
mat_idx_per_sample = np.asarray(mat_idx_list)
T_per_sample = np.asarray(T_list)


# ===== 7. åŸºç¡€æŸå¤±å‡½æ•° =====
def loss_sum_three_parts(y_exp_true, y_exp_pred,
                         Cp1_true, Cp2_true,
                         Cp1_pred, Cp2_pred,
                         slope_true, slope_pred,
                         w1, w2, w3):
    L_exp = np.sum(np.abs(y_exp_true - y_exp_pred))
    L_ref = np.sum(np.abs(Cp1_true - Cp1_pred)) + np.sum(np.abs(Cp2_true - Cp2_pred))
    L_slope = np.sum(np.abs(slope_true - slope_pred))
    return w1 * L_exp + w2 * L_ref + w3 * L_slope


# ===== 8. ä¿®æ­£ï¼šåŸºäºåæ¯”ä¾‹çš„è‡ªé€‚åº”æƒé‡è®¡ç®— =====
def calculate_adaptive_weights_inverse(X_exp, y_exp, Cp1_true, Cp2_true, slope_true):
    """è®¡ç®—æ¯ä¸ªæŸå¤±é¡¹çš„æ•°é‡çº§å¹¶ä½¿ç”¨åæ¯”ä¾‹è°ƒæ•´æƒé‡"""
    L_exp_typical = np.mean(np.abs(y_exp - np.mean(y_exp)))  # å®éªŒç‚¹çš„æ•°é‡vf ckçº§
    L_ref_typical = (np.mean(np.abs(Cp1_true - np.mean(Cp1_true))) +
                     np.mean(np.abs(Cp2_true - np.mean(Cp2_true)))) / 2  # å‚è€ƒç‚¹çš„æ•°é‡çº§
    L_slope_typical = np.mean(np.abs(slope_true - np.mean(slope_true)))  # æ–œç‡çš„æ•°é‡çº§

    L_exp_typical = max(L_exp_typical, 1e-10)
    L_ref_typical = max(L_ref_typical, 1e-10)
    L_slope_typical = max(L_slope_typical, 1e-10)

    print(f"æŸå¤±é¡¹å…¸å‹å€¼: å®éªŒç‚¹={L_exp_typical:.2f}, å‚è€ƒç‚¹={L_ref_typical:.2f}, æ–œç‡={L_slope_typical:.6f}")

    # ä½¿ç”¨åæ¯”ä¾‹å…³ç³»ï¼šæ•°å€¼è¶Šå°ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ï¼ˆæ”¾å¤§ä½œç”¨ï¼‰
    w1_base = 1.0/ L_exp_typical  # å®éªŒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
    w2_base = 1.0 / L_ref_typical  # å‚è€ƒç‚¹æ•°å€¼å¤§ï¼Œæƒé‡å°
    w3_base = 1.0 / L_slope_typical  # æ–œç‡æ•°å€¼å°ï¼Œæƒé‡å¤§ï¼ˆæ”¾å¤§ï¼ï¼‰

    # å½’ä¸€åŒ–
    total_base = w1_base + w2_base + w3_base
    w1_normalized = w1_base / total_base
    w2_normalized = w2_base / total_base
    w3_normalized = w3_base / total_base

    print(f"åæ¯”ä¾‹æƒé‡: w1={w1_normalized:.4f}, w2={w2_normalized:.4f}, w3={w3_normalized:.4f}")

    return w1_normalized, w2_normalized, w3_normalized


# ===== 8.1 è®¡ç®—æŸå¤±å€æ•°ï¼ˆç”¨äºæŸå¤±å‡½æ•°å†…éƒ¨æ”¾å¤§ï¼‰ =====
def calculate_loss_multipliers(L_exp, L_ref, L_slope, max_multiplier=1000):
    """è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°"""
    max_loss = max(L_exp, L_ref, L_slope)

    # è®¡ç®—éœ€è¦æ”¾å¤§çš„å€æ•°
    multiplier_exp = max_loss / L_exp
    multiplier_ref = max_loss / L_ref
    multiplier_slope = max_loss / L_slope

    # é™åˆ¶æœ€å¤§å€æ•°ï¼Œé¿å…æç«¯å€¼
    multiplier_exp = min(multiplier_exp, max_multiplier)
    multiplier_ref = min(multiplier_ref, max_multiplier)
    multiplier_slope = min(multiplier_slope, max_multiplier)

    print(f"æŸå¤±æ”¾å¤§å€æ•°: å®éªŒç‚¹Ã—{multiplier_exp:.2f}, å‚è€ƒç‚¹Ã—{multiplier_ref:.2f}, æ–œç‡Ã—{multiplier_slope:.2f}")

    return multiplier_exp, multiplier_ref, multiplier_slope


# ===== 7.1 ä¿®æ­£ï¼šä½¿ç”¨æ”¾å¤§å€æ•°çš„æŸå¤±å‡½æ•° =====
def loss_sum_three_parts_with_multipliers(y_exp_true, y_exp_pred,
                                          Cp1_true, Cp2_true,
                                          Cp1_pred, Cp2_pred,
                                          slope_true, slope_pred,
                                          w1, w2, w3):
    """ä½¿ç”¨æ”¾å¤§å€æ•°åçš„æŸå¤±å‡½æ•°"""
    L_exp = np.sum(np.abs(y_exp_true - y_exp_pred)) * multiplier_exp
    L_ref = (np.sum(np.abs(Cp1_true - Cp1_pred)) +
             np.sum(np.abs(Cp2_true - Cp2_pred))) * multiplier_ref
    L_slope = np.sum(np.abs(slope_true - slope_pred)) * multiplier_slope

    return w1 * L_exp + w2 * L_ref + w3 * L_slope


# è®¡ç®—åæ¯”ä¾‹åŸºå‡†æƒé‡
base_w1, base_w2, base_w3 = calculate_adaptive_weights_inverse(
    X_exp, y_exp, Cp1_true_all, Cp2_true_all, slope_true_all
)

# è®¡ç®—æŸå¤±æ”¾å¤§å€æ•°
L_exp, L_ref, L_slope = (
    np.mean(np.abs(y_exp - np.mean(y_exp))),
    (np.mean(np.abs(Cp1_true_all - np.mean(Cp1_true_all))) +
     np.mean(np.abs(Cp2_true_all - np.mean(Cp2_true_all)))) / 2,
    np.mean(np.abs(slope_true_all - np.mean(slope_true_all)))
)
multiplier_exp, multiplier_ref, multiplier_slope = calculate_loss_multipliers(L_exp, L_ref, L_slope)

# ===== 9. å¤–å±‚ä¼˜åŒ–ï¼ˆéšæœºé‡‡æ ·å¹¶ç»“åˆè‡ªé€‚åº”åŸºå‡†æƒé‡ï¼‰ =====
candidate_ws = sample_weight_triplets(n=10, seed=2025)  # åªæµ‹è¯•10ç»„æƒé‡

best_w = None
best_r2 = -np.inf
best_theta = None


# ===== 9.1 å†…å±‚ä¼˜åŒ–å‡½æ•°å®šä¹‰ =====
def fit_inner_linear_model(w, X_exp_train, y_exp_train, mat_idx_train,
                           Cp1_true_train, Cp2_true_train, slope_true_train,
                           T1_hat_all, T2_hat_all):
    w1, w2, w3 = w
    n_feat = X_exp_train.shape[1]
    theta0 = np.zeros(n_feat + 1)  # åˆå§‹å…¨0

    def objective(theta):
        beta = theta[:-1]
        b = theta[-1]

        # å®éªŒç‚¹é¢„æµ‹
        y_pred_train = X_exp_train @ beta + b

        # å‚è€ƒç‚¹é¢„æµ‹ï¼ˆæ­£ç¡®çš„è®¡ç®—æ–¹å¼ï¼‰
        unique_materials = np.unique(mat_idx_train)
        Cp1_pred_train = np.zeros(len(unique_materials))
        Cp2_pred_train = np.zeros(len(unique_materials))

        for i, mat_idx in enumerate(unique_materials):
            Nk = X_groups.iloc[mat_idx].values.astype(float)
            s_feat = slope_feat_all[mat_idx]

            x_T1 = np.concatenate([Nk, Nk * T1_hat_all[mat_idx], [s_feat * T1_hat_all[mat_idx]]])
            x_T2 = np.concatenate([Nk, Nk * T2_hat_all[mat_idx], [s_feat * T2_hat_all[mat_idx]]])

            Cp1_pred_train[i] = x_T1 @ beta + b
            Cp2_pred_train[i] = x_T2 @ beta + b

        # æ–œç‡é¢„æµ‹
        slope_pred_train = (Cp2_pred_train - Cp1_pred_train) / (
                T2_hat_all[unique_materials] - T1_hat_all[unique_materials])

        # ä½¿ç”¨å¸¦æ”¾å¤§å€æ•°çš„æŸå¤±å‡½æ•°
        return loss_sum_three_parts_with_multipliers(
            y_exp_true=y_exp_train, y_exp_pred=y_pred_train,
            Cp1_true=Cp1_true_train[unique_materials],
            Cp2_true=Cp2_true_train[unique_materials],
            Cp1_pred=Cp1_pred_train, Cp2_pred=Cp2_pred_train,
            slope_true=slope_true_train[unique_materials],
            slope_pred=slope_pred_train,
            w1=w1, w2=w2, w3=w3
        )

    res = minimize(objective, theta0, method="Powell", options={"maxiter": 5000, "xtol": 1e-6, "ftol": 1e-6})
    return res


# ===== 9.2 å¤–å±‚ä¼˜åŒ–å¾ªç¯ =====
for i, w in enumerate(candidate_ws):
    # ä½¿ç”¨å¹‚å‡½æ•°è¿›ä¸€æ­¥æ”¾å¤§æ–œç‡çš„é‡è¦æ€§ï¼ˆå¦‚æœåŸºå‡†æƒé‡æ˜¾ç¤ºæ–œç‡å¾ˆé‡è¦ï¼‰
    # æ–œç‡åŸºå‡†æƒé‡è¶Šå¤§ï¼Œè¯´æ˜è¶Šéœ€è¦é‡è§†ï¼Œè¿›ä¸€æ­¥æ”¾å¤§
    slope_emphasis = base_w3 ** 0.5  # å¼€å¹³æ–¹æ ¹ï¼Œé¿å…è¿‡åº¦æ”¾å¤§

    adjusted_w = [
        w[0] * 100*base_w1,
        w[1] * base_w2,
        w[2] * base_w3 # é¢å¤–æ”¾å¤§æ–œç‡æƒé‡
        # w[2] * base_w3 * (1 + slope_emphasis)  # é¢å¤–æ”¾å¤§æ–œç‡æƒé‡
    ]

    # å½’ä¸€åŒ–è°ƒæ•´åçš„æƒé‡
    total_w = sum(adjusted_w)
    adjusted_w = [w / total_w for w in adjusted_w]

    print(f"\nç¬¬{i + 1}ç»„è°ƒæ•´åæƒé‡: {[f'{x:.6f}' for x in adjusted_w]}")

    res = fit_inner_linear_model(
        w=adjusted_w,
        X_exp_train=X_exp,
        y_exp_train=y_exp,
        mat_idx_train=mat_idx_per_sample,
        Cp1_true_train=Cp1_true_all,
        Cp2_true_train=Cp2_true_all,
        slope_true_train=slope_true_all,  # ä½¿ç”¨çœŸå®æ–œç‡è€Œä¸æ˜¯é¢„æµ‹æ–œç‡
        T1_hat_all=T1_hat_all,
        T2_hat_all=T2_hat_all
    )

    if not res.success:
        print(f"è­¦å‘Š: ç¬¬ {i + 1} ç»„æƒé‡ä¼˜åŒ–å¤±è´¥: {res.message}")
        continue

    theta = res.x
    beta, b = theta[:-1], theta[-1]

    y_val_pred = X_exp @ beta + b
    r2 = r2_score(y_exp, y_val_pred)

    print(f"æƒé‡ {[f'{x:.6f}' for x in adjusted_w]} -> RÂ² = {r2:.6f}")

    if r2 > best_r2:
        best_r2 = r2
        best_w = adjusted_w
        best_theta = theta

print(f"\nå¤–å±‚æœ€ä¼˜æƒé‡ w* = {best_w}, éªŒè¯é›† RÂ² = {best_r2:.6f}")

# ===== 10. ç”¨æœ€ä¼˜æƒé‡é‡è®­ =====
print("\nä½¿ç”¨æœ€ä¼˜æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ...")
res_final = fit_inner_linear_model(
    w=best_w,
    X_exp_train=X_exp,
    y_exp_train=y_exp,
    mat_idx_train=mat_idx_per_sample,
    Cp1_true_train=Cp1_true_all,
    Cp2_true_train=Cp2_true_all,
    slope_true_train=slope_true_all,
    T1_hat_all=T1_hat_all,
    T2_hat_all=T2_hat_all
)

theta_final = res_final.x
beta_final, b_final = theta_final[:-1], theta_final[-1]

# ===== 11. è¯„ä¼°ç»“æœ =====
y_pred_all = X_exp @ beta_final + b_final
mse_all = mean_squared_error(y_exp, y_pred_all)
r2_all = r2_score(y_exp, y_pred_all)

rel_err = np.abs((y_pred_all - y_exp) / np.where(np.abs(y_exp) < 1e-12, 1e-12, y_exp)) * 100

# ===== 12. å¯¼å‡ºç»“æœ =====
results = pd.DataFrame({
    "Material_ID": material_ids_all[mat_idx_per_sample],
    "Temperature (K)": T_per_sample,
    "Cp_measured": y_exp,
    "Cp_predicted": y_pred_all,
    "Relative_Error_%": rel_err
})
results.to_excel("Cpé¢„æµ‹ç»“æœ_ä¼˜åŒ–å.xlsx", index=False)

feature_labels = list(group_cols) + [f"{g}_T" for g in group_cols] + ["slopeÃ—T"]
coef_df = pd.DataFrame({"Feature": feature_labels, "Contribution": beta_final})
coef_df.to_excel("Cpç³»æ•°è¡¨_ä¼˜åŒ–å.xlsx", index=False)

# ===== 13. æœ€ç»ˆç»“æœæ±‡æ€» =====
print("\n" + "=" * 60)
print("ğŸ¯ æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€»")
print("=" * 60)
print(f"æœ€ä¼˜æƒé‡ç»„åˆ: w1={best_w[0]:.6f}, w2={best_w[1]:.6f}, w3={best_w[2]:.6f}")
print(f"æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
print(f"  RÂ²  = {r2_all:.6f}")
print(f"  MSE = {mse_all:.6f}")
print(f"  â‰¤1%: {(rel_err <= 1).sum()}/{len(rel_err)} ({(rel_err <= 1).sum() / len(rel_err) * 100:.2f}%)")
print(f"  â‰¤5%: {(rel_err <= 5).sum()}/{len(rel_err)} ({(rel_err <= 5).sum() / len(rel_err) * 100:.2f}%)")
print(f"  â‰¤10%: {(rel_err <= 10).sum()}/{len(rel_err)} ({(rel_err <= 10).sum() / len(rel_err) * 100:.2f}%)")

# è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®
mean_rel_err = np.mean(rel_err)
median_rel_err = np.median(rel_err)
print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_err:.2f}%")
print(f"  ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®: {median_rel_err:.2f}%")

# è®¡ç®—RÂ²_adjusted
n_samples = len(y_exp)
n_features = X_exp.shape[1]
r2_adjusted = 1 - (1 - r2_all) * (n_samples - 1) / (n_samples - n_features - 1)
print(f"  è°ƒæ•´åRÂ²: {r2_adjusted:.6f}")

print("=" * 60)
print("âœ… å®Œæˆï¼é¢„æµ‹ç»“æœå’Œç³»æ•°è¡¨å·²ä¿å­˜åˆ°Excelæ–‡ä»¶")